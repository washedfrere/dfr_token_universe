{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:grey;\"> Tokenizador de textos </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Memoria: svmem(total=12649299968, available=7030190080, percent=44.4, used=5619109888, free=7030190080)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import psutil\n",
    "import pickle\n",
    "from math import log\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dfrtokenuniverse.splitter\n",
    "import dfrtokenuniverse.word_inventory\n",
    "import dfrtokenuniverse.constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recarga librerías propias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dfrtokenuniverse.splitter)\n",
    "importlib.reload(dfrtokenuniverse.word_inventory)\n",
    "importlib.reload(dfrtokenuniverse.constantes)\n",
    "from dfrtokenuniverse.splitter import TextSplitter\n",
    "from dfrtokenuniverse.constantes import KDfrNlp\n",
    "K = KDfrNlp()\n",
    "splitter = TextSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diccionario de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo D:\\datos\\tokens_ttkn_id_v20241208.pkl\n",
      "Nada más leer:\n",
      "6 Del Sistema 9 ['<', '|', 'v', 'a', 'i', 'd', '2', '|', '>']\n",
      "1 Letras 14137 ['bar', 'bom', 'bo', 'deo', 'eo', 'de', 'ser', 'ivk', 'el', 'rr']\n",
      "2 Separadores 359 [' , ', ' ,', '  ', '. ', ', ', '   ', ' .', ' . ', ' \"', '\".']\n",
      "0 Dígitos 920 ['22', '20', '01', '00', '21', '12', '16', '38', '020', '521']\n",
      "3 Símbolos 227 ['}}', '((', '))', '])', '>>', '}{', '})', ')}', ']}', '<{']\n",
      "5 Desconocidos 5744 ['ɾ', 'ˈ', 'ː', 'ž', 'č', '\\u200b', '«', '»', '“', '”']\n",
      "4 Especiales 8 ['\\t', '\\t\\t', '\\t\\t\\t', '\\x08', '\\x07', '\\x0c', '\\r', '\\x0b']\n",
      "Arreglado:\n",
      "6 Del Sistema 10 ['<|PAD|>', '<|UPPER|>', '<|LOWER|>', '<|CAP|>', '<|START|>', '<|TBC|>', '<|EOF|>', '<|UNK|>', '<|void1|>', '<|vaid2|>']\n",
      "1 Letras 14137 ['bar', 'bom', 'bo', 'deo', 'eo', 'de', 'ser', 'ivk', 'el', 'rr']\n",
      "2 Separadores 359 [' , ', ' ,', '  ', '. ', ', ', '   ', ' .', ' . ', ' \"', '\".']\n",
      "0 Dígitos 920 ['22', '20', '01', '00', '21', '12', '16', '38', '020', '521']\n",
      "3 Símbolos 227 ['}}', '((', '))', '])', '>>', '}{', '})', ')}', ']}', '<{']\n",
      "5 Desconocidos 5744 ['ɾ', 'ˈ', 'ː', 'ž', 'č', '\\u200b', '«', '»', '“', '”']\n",
      "4 Especiales 8 ['\\t', '\\t\\t', '\\t\\t\\t', '\\x08', '\\x07', '\\x0c', '\\r', '\\x0b']\n"
     ]
    }
   ],
   "source": [
    "tokens_path = r\"D:\\datos\\tokens_ttkn_id_v20241208.pkl\"\n",
    "print(f\"Leyendo {tokens_path}\")\n",
    "with open(tokens_path, \"rb\") as file:\n",
    "    tokens_dict = pickle.load(file)\n",
    "print(\"Nada más leer:\")\n",
    "for ttkn in tokens_dict:\n",
    "    print(ttkn, K.TTKN_DESC[ttkn], len(tokens_dict[ttkn]), [x for x in tokens_dict[ttkn]][:10])\n",
    "tokens_dict[K.TTKN_SIS] = {token:K.MAIN_TOKEN_DICT[token] for token in K.MAIN_TOKEN_DICT}\n",
    "print(\"Arreglado:\")\n",
    "for ttkn in tokens_dict:\n",
    "    print(ttkn, K.TTKN_DESC[ttkn], len(tokens_dict[ttkn]), [x for x in tokens_dict[ttkn]][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos por ttkn=6 (Del Sistema)\n",
      "Vamos por ttkn=1 (Letras)\n",
      "Vamos por ttkn=2 (Separadores)\n",
      "Vamos por ttkn=0 (Dígitos)\n",
      "Vamos por ttkn=3 (Símbolos)\n",
      "Vamos por ttkn=5 (Desconocidos)\n",
      "Vamos por ttkn=4 (Especiales)\n",
      "El máximo token_id es 95743 log: 5 -> 10^x = 100000\n",
      "Generados 21405 tokens_id a reales\n",
      "2 2e-05 2 {'token': '<|LOWER|>', 'ttkn': 6}\n",
      "51 0.00051 51 {'token': '\\t\\t', 'ttkn': 4}\n",
      "1230 0.0123 1230 {'token': '\",\"', 'ttkn': 2}\n",
      "5132 0.05132 5132 {'token': '$)', 'ttkn': 3}\n",
      "41235 0.41235 41235 {'token': 'ner', 'ttkn': 1}\n",
      "93256 0.93256 93256 {'token': 'ṯ', 'ttkn': 5}\n"
     ]
    }
   ],
   "source": [
    "# Recuperamos la informacion de tipo de token (ttkn) junto al token\n",
    "ids_token = {}\n",
    "MAX_TOKEN_ID = 0\n",
    "for ttkn in tokens_dict:\n",
    "    print(f\"Vamos por ttkn={ttkn} ({K.TTKN_DESC[ttkn]})\")\n",
    "    for token in tokens_dict[ttkn]:\n",
    "        token_id = tokens_dict[ttkn][token]\n",
    "        ids_token[token_id] = {\"token\": token, \"ttkn\": ttkn}\n",
    "        if token_id > MAX_TOKEN_ID:\n",
    "            MAX_TOKEN_ID = tokens_dict[ttkn][token]\n",
    "print(f\"El máximo token_id es {MAX_TOKEN_ID} log: {round(log(MAX_TOKEN_ID, 10))} -> 10^x = {10**round(log(MAX_TOKEN_ID, 10))}\")\n",
    "TOKEN_DIVISOR = 10**round(log(MAX_TOKEN_ID, 10))\n",
    "# Generamos los valores reales\n",
    "token_id_real = {}\n",
    "for token_id in ids_token:\n",
    "    token_id_real[token_id] = round(token_id / TOKEN_DIVISOR, 7)\n",
    "print(f\"Generados {len(token_id_real)} tokens_id a reales\")\n",
    "def token_real_id(token_real):\n",
    "    token_id = int(round(token_real, 7) * TOKEN_DIVISOR)\n",
    "    if token_id in token_id_real:\n",
    "        return token_id\n",
    "    else:\n",
    "        if token_id + 1 in token_id_real and token_id_real[token_id+1] == round(token_real,7):\n",
    "            return token_id + 1\n",
    "        else:\n",
    "            return token_id - 1\n",
    "#Samples\n",
    "for token_id in [2, 51, 1230, 5132, 41235, 93256]:\n",
    "    while token_id not in token_id_real:\n",
    "        token_id += 1\n",
    "        if token_id > MAX_TOKEN_ID:\n",
    "            break\n",
    "    treal = token_id_real[token_id]\n",
    "    print(token_id, treal, token_real_id(treal), ids_token[token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40999, 40014]\n",
      "['ho', 'la']\n",
      "Hola\n",
      "HOLA\n"
     ]
    }
   ],
   "source": [
    "def tokeniza(n_grama, ttkn):\n",
    "    global tokens_dict\n",
    "    tokenizado = []\n",
    "    for token in tokens_dict[ttkn]:\n",
    "        if token in n_grama:\n",
    "            for subn in n_grama.split(token):\n",
    "                if subn and subn > \"\":\n",
    "                    tokenizado.extend(tokeniza(subn, ttkn))\n",
    "                tokenizado.append(tokens_dict[ttkn][token])\n",
    "            tokenizado.pop(-1)\n",
    "            break\n",
    "    return tokenizado\n",
    "# Test\n",
    "print(tokeniza(\"hola\", 1))\n",
    "print([ids_token[token_id][\"token\"] for token_id in tokeniza(\"hola\", 1)])\n",
    "\n",
    "def detokeniza(tokenizado):\n",
    "    global ids_token\n",
    "    detokenizado = \"\"\n",
    "    token_sis = \"\"\n",
    "    for token_ir in tokenizado:\n",
    "        \n",
    "        token = \"\"\n",
    "        try:\n",
    "            if token_ir < 1.0:\n",
    "                token_id = token_real_id(token_ir)\n",
    "            else:\n",
    "                token_id = token_ir\n",
    "            if ids_token[token_id][\"ttkn\"] == K.TTKN_SIS:\n",
    "                token_sis = ids_token[token_id][\"token\"]\n",
    "            else:\n",
    "                token = ids_token[token_id][\"token\"]\n",
    "                if token_sis == K.UPPER_TOKEN:\n",
    "                    token = \"\" + token.upper()\n",
    "                elif token_sis == K.CAP_TOKEN:\n",
    "                    token = token[0].upper() + token[1:]\n",
    "                token_sis = \"\"\n",
    "        except Exception as e:\n",
    "            token = \"!\"\n",
    "        detokenizado += token\n",
    "    return detokenizado\n",
    "# Test\n",
    "print(detokeniza([3, 40999, 40014]))\n",
    "print(detokeniza([1, 40999, 1, 40014]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 40061, 54104, 1329, 40109, 1329, 40018, 54103, 1004, 40152, 54111, 1329, 54104, 40142, 1329, 5078, 40180, 5079, 1338, 3, 54119, 40070, 54112, 1329, 40061, 54103, 1329, 40109, 1329, 40018, 54103, 1329, 40036, 1329, 5083, 1, 5076, 1329, 20059, 20187, 20916, 1335, 20072]\n",
      "[3e-05, 0.40061, 0.54104, 0.01329, 0.40109, 0.01329, 0.40018, 0.54103, 0.01004, 0.40152, 0.54111, 0.01329, 0.54104, 0.40142, 0.01329, 0.05078, 0.4018, 0.05079, 0.01338, 3e-05, 0.54119, 0.4007, 0.54112, 0.01329, 0.40061, 0.54103, 0.01329, 0.40109, 0.01329, 0.40018, 0.54103, 0.01329, 0.40036, 0.01329, 0.05083, 1e-05, 0.05076, 0.01329, 0.20059, 0.20187, 0.20916, 0.01335, 0.20072]\n",
      "['<|CAP|>', 'est', 'o', ' ', 'es', ' ', 'un', 'a', ', ', 'ha', 'y', ' ', 'o', 'tra', ' ', '¿', 'no', '?', '\\n', '<|CAP|>', 'p', 'ue', 's', ' ', 'est', 'a', ' ', 'es', ' ', 'un', 'a', ' ', 'más', ' ', '¡', '<|UPPER|>', '!', ' ', '23', '58', '7', ',', '33']\n",
      "Esto es una, hay otra ¿no?\n",
      "Pues esta es una más ¡! 23587,33\n",
      "448\n",
      "Memoria: svmem(total=12649299968, available=6340509696, percent=49.9, used=6308790272, free=6340509696)\n"
     ]
    }
   ],
   "source": [
    "tokenizado = []\n",
    "texto = \"Esto es una, hay otra ¿no?\\nPues esta es una más ¡SEGURO! 23587,33\"\n",
    "for ttkn, n_grama in splitter.split_by_type(texto, 7):\n",
    "    valores = splitter.normalize_n_gram(n_grama)\n",
    "    if len(valores) > 1:\n",
    "        tokenizado.append(K.MAIN_TOKEN_DICT[valores[0]])\n",
    "    tokenizado.extend(tokeniza(valores[-1], ttkn))\n",
    "print(tokenizado)\n",
    "print([token_id_real[x] for x in tokenizado])\n",
    "print([ids_token[token_id][\"token\"] for token_id in tokenizado])\n",
    "print(detokeniza(tokenizado))\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo desde: D:\\datos\\wiki_1909627_entradas.pkl\n",
      "0\n",
      "Memoria: svmem(total=12649299968, available=1524199424, percent=88.0, used=11125100544, free=1524199424)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "wiki_path = r\"D:\\datos\\wiki_1909627_entradas.pkl\"\n",
    "print(f\"Leyendo desde: {wiki_path}\")\n",
    "with open(wiki_path, \"rb\") as wiki_file:\n",
    "    wiki_data = pickle.load(wiki_file)\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos en wiki: 1909627\n"
     ]
    }
   ],
   "source": [
    "print(f\"Documentos en wiki: {len(wiki_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2>Tokenizando Texto</h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_tokenized = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltar  = 0\n",
    "recoge = 10000 # Entradas de la wikipedia con sus textos\n",
    "limite = saltar + recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Realizamos varias pasadas para ir acumulando n-gramas</h3>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llevamos 10000 entradas de 10000 (100.0%)\n",
      "Con 16383634 tokens generados\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recogemos {recoge} entradas a partir de {saltar}\")\n",
    "cont_entradas = 0\n",
    "cont_tokens = 0\n",
    "max_len_allowed=7\n",
    "for entrada in wiki_data:\n",
    "    if cont_entradas > saltar:\n",
    "        texto_tokenized[cont_entradas] = []\n",
    "        tokenizado = []\n",
    "        for ttkn, n_grama in splitter.split_by_type(entrada, max_len_allowed):\n",
    "            valores = splitter.normalize_n_gram(n_grama)\n",
    "            if len(valores) > 1:\n",
    "                tokenizado.append(K.MAIN_TOKEN_DICT[valores[0]])\n",
    "            tokenizado.extend(tokeniza(valores[-1], ttkn))\n",
    "        texto_tokenized[cont_entradas].append([token_id_real[x] for x in tokenizado])\n",
    "        cont_tokens += len(tokenizado)\n",
    "        for linea in wiki_data[entrada]:\n",
    "            tokenizado = []\n",
    "            for ttkn, n_grama in splitter.split_by_type(linea.decode(\"utf-8\"), max_len_allowed):\n",
    "                valores = splitter.normalize_n_gram(n_grama)\n",
    "                if len(valores) > 1:\n",
    "                    tokenizado.append(K.MAIN_TOKEN_DICT[valores[0]])\n",
    "                tokenizado.extend(tokeniza(valores[-1], ttkn))\n",
    "            texto_tokenized[cont_entradas].append([token_id_real[x] for x in tokenizado])\n",
    "            cont_tokens += len(tokenizado)\n",
    "        if cont_entradas > limite:\n",
    "            break\n",
    "        if cont_entradas % 1000 == 0:\n",
    "            clear_output(True)\n",
    "            print(f\"Llevamos {cont_entradas} entradas de {limite} ({round(100*cont_entradas/limite, 2)}%)\")\n",
    "            print(f\"Con {cont_tokens} tokens generados\")\n",
    "    cont_entradas += 1\n",
    "salta = limite\n",
    "limite += recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px dashed #aaa;\"/>\n",
    "<center><h2>Liberamos memoria</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria: svmem(total=12649299968, available=3535872000, percent=72.0, used=9113427968, free=3535872000)\n",
      "187\n",
      "Memoria: svmem(total=12649299968, available=3725950976, percent=70.5, used=8923348992, free=3725950976)\n"
     ]
    }
   ],
   "source": [
    "print(\"Memoria:\", psutil.virtual_memory())\n",
    "del wiki_data\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Memoria: svmem(total=12649299968, available=7463751680, percent=41.0, used=5185548288, free=7463751680)\n"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando D:\\datos\\texto_tokenizado_v20241208.pkl\n"
     ]
    }
   ],
   "source": [
    "tokens_path = r\"D:\\datos\\texto_tokenizado_v20241208.pkl\"\n",
    "print(f\"Guardando {tokens_path}\")\n",
    "with open(tokens_path, \"wb\") as file:\n",
    "    pickle.dump(texto_tokenized, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo D:\\datos\\texto_tokenizado_v20241208.pkl\n"
     ]
    }
   ],
   "source": [
    "tokens_path = r\"D:\\datos\\texto_tokenizado_v20241208.pkl\"\n",
    "print(f\"Leyendo {tokens_path}\")\n",
    "with open(tokens_path, \"rb\") as file:\n",
    "    texto_tokenized = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color: red; height: 2; border: 3px dotted;\"/>\n",
    "<center><h1 style=\"color: yellow;\">Comienza la fiesta</h1></center>\n",
    "<hr style=\"color: red; width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El primer caracter DIGITO está en el token-r = 0.2\n",
      "El primer caracter LETRA está en el token-r = 0.4\n",
      "El primer caracter DESCONOCIDO está en el token-r = 0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FIRST_DIG_TOKEN_R = 0\n",
    "for token in tokens_dict[K.TTKN_DIG]:\n",
    "    FIRST_DIG_TOKEN_R = token_id_real[tokens_dict[K.TTKN_DIG][token]]\n",
    "    break\n",
    "print(f\"El primer caracter DIGITO está en el token-r = {FIRST_DIG_TOKEN_R}\")\n",
    "FIRST_LET_TOKEN_R = 0\n",
    "for token in tokens_dict[K.TTKN_LET]:\n",
    "    FIRST_LET_TOKEN_R = token_id_real[tokens_dict[K.TTKN_LET][token]]\n",
    "    break\n",
    "print(f\"El primer caracter LETRA está en el token-r = {FIRST_LET_TOKEN_R}\")\n",
    "FIRST_UNK_TOKEN_R = 0\n",
    "for token in tokens_dict[K.TTKN_UNK]:\n",
    "    FIRST_UNK_TOKEN_R = token_id_real[tokens_dict[K.TTKN_UNK][token]]\n",
    "    break\n",
    "print(f\"El primer caracter DESCONOCIDO está en el token-r = {FIRST_UNK_TOKEN_R}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de texto para entrenamiento predictivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4e-05, 3e-05], [4e-05, 0.40001], [4e-05, 0.4], [4e-05, 0.54101], [4e-05, 0.40004], [4e-05, 3e-05]\n"
     ]
    }
   ],
   "source": [
    "class GetNextToken():\n",
    "    \"\"\"Clase para manejar la generacion de datos de entrenamiento\n",
    "    Controla una lista de listas de vectores organizada de la siquiente manera:\n",
    "    - nivel 0: articulo\n",
    "    - nivel 1: linea 0 = titulo\n",
    "    - nivel 1: linea 1+ = contenido\n",
    "    - nivel 2: token_r\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.art_id = 1\n",
    "        self.i_l = 0\n",
    "        self.i_c = 0\n",
    "        self.log = []\n",
    "    def reset(self):\n",
    "        self.art_id = 1\n",
    "        self.i_l = 0\n",
    "        self.i_c = 0\n",
    "        self.log = []\n",
    "    def take_next(self, tokenizado):\n",
    "        global token_id_real, K\n",
    "        if self.art_id == 1 or self.i_l == 0 or self.i_c == 0:\n",
    "            if len(tokenizado[self.art_id][self.i_l]) > 1:\n",
    "                next_token = [\n",
    "                    token_id_real[K.MAIN_TOKEN_DICT[K.START_TOKEN]],\n",
    "                    tokenizado[self.art_id][self.i_l][self.i_c]\n",
    "                ]\n",
    "            else:\n",
    "                next_token = []\n",
    "        else:\n",
    "            self.log.append(f\"(A) art_id={self.art_id}, i_l={self.i_l}, i_c={self.i_c}\")\n",
    "            if len(tokenizado[self.art_id][self.i_l]) > 1:\n",
    "                next_token = [tokenizado[self.art_id][self.i_l][self.i_c]]\n",
    "            else:\n",
    "                next_token = []\n",
    "        self.i_c += 1\n",
    "        if self.i_c >= len(tokenizado[self.art_id][self.i_l]):\n",
    "            self.i_c = 0\n",
    "            self.i_l += 1\n",
    "            if self.i_l >= len(tokenizado[self.art_id]):\n",
    "                self.log = [f\"(B) art_id={self.art_id}, i_l={self.i_l}, i_c={self.i_c}\"]\n",
    "                self.i_l = 0\n",
    "                self.art_id += 1\n",
    "                if self.art_id >= len(tokenizado):\n",
    "                    self.log = []\n",
    "                    self.art_id = 1\n",
    "                    next_token.append(token_id_real[K.MAIN_TOKEN_DICT[K.EOF_TOKEN]])\n",
    "                else:\n",
    "                    self.log.append(f\"(C) art_id={self.art_id}, i_l={self.i_l}, i_c={self.i_c}\")\n",
    "                    next_token.append(token_id_real[K.MAIN_TOKEN_DICT[K.CONTINUE_TOKEN]])\n",
    "            else:\n",
    "                self.log.append(f\"(D) art_id={self.art_id}, i_l={self.i_l}, i_c={self.i_c}\")\n",
    "                next_token.append(token_id_real[K.MAIN_TOKEN_DICT[K.CONTINUE_TOKEN]])\n",
    "        return next_token\n",
    "# TEst\n",
    "gnt = GetNextToken()\n",
    "for i in range(5):\n",
    "    print(gnt.take_next(texto_tokenized), end=\", \")\n",
    "gnt.reset()\n",
    "print(gnt.take_next(texto_tokenized))\n",
    "gnt.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de datos\n",
    "El objetivo es aprender a predecir el siguiente token dados unos de partida.\n",
    "- Features: vector de tokens\n",
    "- Targets: vector de tokens corrido a la izquierda más un token nuevo\n",
    "\n",
    "La característica principal de este set de datos es que rellena con ceros por la izquierda para:\n",
    "- Entrenar mejor las neuronas más cercanas al nuevo token\n",
    "- Favorecer que las neuronas menos usadas queden lejos del rango de cálculo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El token <|PAD|> es : 0.0\n",
      "Finalizado! con límite de {data_limit} para vectores de {NUM_CONTEXT_TOKENS}\n",
      "Tenemos 12330 features y 12330 targets\n"
     ]
    }
   ],
   "source": [
    "NUM_CONTEXT_TOKENS = 50\n",
    "data_limit = 10000\n",
    "min_info = 5 # numero de tokens minimo para entrenar\n",
    "min_ceros = data_limit - min_info\n",
    "X = []\n",
    "Y = []\n",
    "pad_token_r = token_id_real[K.MAIN_TOKEN_DICT[K.PAD_TOKEN]]\n",
    "eof_token_r = token_id_real[K.MAIN_TOKEN_DICT[K.EOF_TOKEN]]\n",
    "cont_token_r = token_id_real[K.MAIN_TOKEN_DICT[K.CONTINUE_TOKEN]]\n",
    "print(f\"El token <|PAD|> es : {pad_token_r}\")\n",
    "gnt.reset()\n",
    "x_vector = [pad_token_r]*NUM_CONTEXT_TOKENS\n",
    "y_vector = [pad_token_r]*NUM_CONTEXT_TOKENS\n",
    "token_actu = gnt.take_next(texto_tokenized)\n",
    "for token_r in token_actu:\n",
    "    x_vector = y_vector.copy()\n",
    "    y_vector = y_vector[1:] + [token_r]\n",
    "cont_data = 1\n",
    "# El proceso termina con fin de datos o con fin de seccion y limite de datos\n",
    "while token_actu[-1] != eof_token_r and not (cont_data >= data_limit and token_actu[-1] == cont_token_r):\n",
    "    # Si el token reciente no es fin de fichero, corremos a la izquierda posicion e incluimos el nuevo\n",
    "    token_actu = gnt.take_next(texto_tokenized)\n",
    "    for token_r in token_actu:\n",
    "        x_vector = y_vector.copy()\n",
    "        y_vector = y_vector[1:] + [token_r]\n",
    "        # Solo generamos cuando un minimo de informacion\n",
    "        if x_vector.count(0.0) < min_ceros:\n",
    "            X.append(x_vector)\n",
    "            Y.append(y_vector)\n",
    "    # Si el token reciente es fin de algo, vaciamos la respuesta ordenadamente\n",
    "    if token_actu[-1] in [cont_token_r, eof_token_r]:\n",
    "        for token_r in token_actu:\n",
    "            x_vector = y_vector.copy()\n",
    "            y_vector = y_vector[1:] + [token_r]\n",
    "            # Solo generamos cuando un minimo de informacion\n",
    "            if x_vector.count(0.0) < min_ceros:\n",
    "                X.append(x_vector)\n",
    "                Y.append(y_vector)\n",
    "        # Y empezamos de nuevo\n",
    "        x_vector = [pad_token_r]*NUM_CONTEXT_TOKENS\n",
    "        y_vector = [pad_token_r]*NUM_CONTEXT_TOKENS\n",
    "    cont_data += 1\n",
    "print(\"Finalizado! con límite de {data_limit} para vectores de {NUM_CONTEXT_TOKENS}\")\n",
    "print(f\"Tenemos {len(X)} features y {len(Y)} targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05]\n",
      "Y:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05]\n",
      "\n",
      "X:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05]\n",
      "Y: bom [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001]\n",
      "\n",
      "X: bom [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001]\n",
      "Y: bom [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05]\n",
      "\n",
      "X: bom [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05]\n",
      "Y: bombar [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4]\n",
      "\n",
      "X: bombar [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4]\n",
      "Y: bombar [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4, 4e-05]\n",
      "\n",
      "X: bombar [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4, 4e-05]\n",
      "Y: bombard [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4, 4e-05, 0.54101]\n",
      "\n",
      "X: bombard [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4, 4e-05, 0.54101]\n",
      "Y: bombard [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4, 4e-05, 0.54101, 4e-05]\n",
      "\n",
      "X: bombard [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4, 4e-05, 0.54101, 4e-05]\n",
      "Y: bombardeo [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4e-05, 3e-05, 4e-05, 0.40001, 4e-05, 0.4, 4e-05, 0.54101, 4e-05, 0.40004]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(\"X:\", detokeniza(X[i]), X[i])\n",
    "    print(\"Y:\", detokeniza(Y[i]), Y[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red a entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Una red autoencoder\n",
    "Porque, además de entrenamiento predictivo, queremos obtener un resultado semántico intermedio como embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura de la red\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=100, encoder_size=1500, encode_layers=[64, 128], generative_layers=[128, 64]):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in encode_layers:\n",
    "            encoder_layers.append(nn.Linear(current_size, hidden_size))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            current_size = hidden_size\n",
    "        encoder_layers.append(nn.Linear(current_size, encoder_size))  # Encoder layer\n",
    "        encoder_layers.append(nn.Sigmoid())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        current_size = encoder_size\n",
    "        for hidden_size in reversed(generative_layers):\n",
    "            decoder_layers.append(nn.Linear(current_size, hidden_size))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            current_size = hidden_size\n",
    "        decoder_layers.append(nn.Linear(current_size, input_size))  # Output layer\n",
    "        decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Entrenamiento\n",
    "def train_autoencoder(model, dataloader, epochs=50, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.6f}\")\n",
    "\n",
    "# Dividir la red en encoder y decoder\n",
    "def split_model(model):\n",
    "    encoder = model.encoder # Este modulo genera embeddings\n",
    "    decoder = model.decoder\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciado del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de parámetros: 563541\n"
     ]
    }
   ],
   "source": [
    "# Datos de entrenamiento\n",
    "batch_size = 16\n",
    "dataset = TensorDataset(\n",
    "    torch.tensor(X, dtype=torch.float32),\n",
    "    torch.tensor(Y, dtype=torch.float32)\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Arquitectura del modelo\n",
    "model = Autoencoder(\n",
    "    input_size=len(X[0]),\n",
    "    encoder_size=1500,  # embeddings\n",
    "    encode_layers=[len(X[0]) // 3, len(X[0]) // 2],\n",
    "    generative_layers=[len(X[0]) * 3 , len(X[0]) * 7, len(X[0]) * 5]\n",
    ")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Número total de parámetros: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.031936\n",
      "Epoch 2, Loss: 0.029767\n",
      "Epoch 3, Loss: 0.029075\n",
      "Epoch 4, Loss: 0.028683\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[128], line 42\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[1;34m(model, dataloader, epochs, lr)\u001b[0m\n\u001b[0;32m     40\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     44\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[128], line 28\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoded)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\dfroi\\Documents\\Gits\\dfr_token_universe\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_autoencoder(model, dataloader, epochs=20, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 ¿De qué manera ruedo rrobar esto?\n",
      "Input: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05077999830245972, 2.9999999242136255e-05, 0.40005001425743103, 0.013290000148117542, 0.4118799865245819, 0.5412300229072571, 0.013290000148117542, 0.40217000246047974, 0.5411700010299683, 0.4029799997806549, 0.013290000148117542, 0.5411900281906128, 0.40070000290870667, 0.4009400010108948, 0.013290000148117542, 0.5411900281906128, 0.40042001008987427, 0.4000000059604645, 0.013290000148117542, 0.4006099998950958, 0.5410400032997131, 0.05079000070691109]\n",
      "Encoded (embeddings): [0.003197272541001439, 0.005459378473460674, 0.0003162634966429323, 0.0007794348057359457, 6.60949299344793e-05, 0.005031181965023279, 0.00026092425105161965, 0.0018234200542792678, 0.0018189524998888373, 0.9152073860168457, 0.0024097347632050514, 0.0001841401244746521, 0.00019020085164811462, 0.0055401199497282505, 0.0009540896862745285, 0.008326197043061256, 0.0020501562394201756, 0.0005983958253636956, 0.005275887902826071, 4.428875763551332e-05, 0.027130095288157463, 0.01675909198820591, 0.002258912194520235, 0.0013985756086185575, 0.0600607767701149, 0.008918596431612968, 0.008967841975390911, 0.006090241018682718, 2.5513465516269207e-05, 0.004107627086341381, 0.0034639814402908087, 0.012559116818010807, 0.0075128101743757725, 0.008054559119045734, 0.002131337532773614, 0.0036476710811257362, 8.635459380457178e-05, 0.0165837574750185, 0.0004967433051206172, 0.0027420292608439922, 0.0038740141317248344, 0.009375019930303097, 0.00015214178711175919, 0.022998226806521416, 0.008240040391683578, 0.006194950547069311, 0.0037441786844283342, 0.028646962717175484, 0.015918906778097153, 0.004504692275077105, 0.019613957032561302, 0.010869201272726059, 0.002286451868712902, 0.0003403767477720976, 0.030887583270668983, 0.10377636551856995, 0.08616114407777786, 0.0004806817159987986, 0.02171524055302143, 0.024534810334444046, 0.0005177633138373494, 0.034691739827394485, 0.0020514249335974455, 0.002570194425061345, 0.007906288839876652, 0.0005373342428356409, 0.0004159556410741061, 0.008266451768577099, 0.016682326793670654, 0.008417835459113121, 0.0028635659255087376, 0.005147406831383705, 0.0059447200037539005, 0.013505167327821255, 0.01971030980348587, 0.00017963224672712386, 0.1366914063692093, 0.007357344031333923, 0.0033238017931580544, 0.008551539853215218, 0.006568598095327616, 0.004347613546997309, 0.016838038340210915, 0.005252073053270578, 0.00018502716557122767, 0.004771914798766375, 0.0013644385617226362, 0.0018546652281656861, 0.07130185514688492, 0.04090478643774986, 0.0006537114968523383, 0.0012523361947387457, 0.004035627469420433, 0.08949076384305954, 0.005490537267178297, 0.01724066771566868, 0.003236117074266076, 0.0024182377383112907, 0.005645231809467077, 0.00014125421876087785, 0.034712161868810654, 0.0028177034109830856, 0.02775309793651104, 0.0003238014760427177, 5.5086016800487414e-05, 0.0007852921262383461, 0.0009992507984861732, 0.012836105190217495, 0.005990154109895229, 0.005055856425315142, 0.00043569429544731975, 0.003175922203809023, 0.0840120017528534, 0.0030311187729239464, 0.0010165955172851682, 0.0001236410898854956, 0.003195008961483836, 0.00024346464488189667, 0.0005447257426567376, 0.0008282641647383571, 0.021278422325849533, 0.002562717767432332, 0.004921155981719494, 0.006974325515329838, 0.006799038499593735, 0.04691072925925255, 0.011556608602404594, 0.02341046929359436, 0.0020772062707692385, 0.007069098297506571, 0.0012955712154507637, 0.0015719173243269324, 0.021805964410305023, 0.0009594945004209876, 0.000211087433854118, 0.0009270465816371143, 0.005356910172849894, 0.00023640342988073826, 0.0005493841599673033, 0.00037370348582044244, 0.001119928085245192, 0.22628335654735565, 0.24409079551696777, 0.00039569908403791487, 0.029163477942347527, 0.033314719796180725, 0.008682196959853172, 0.036753494292497635, 0.00029546316363848746, 0.001415996695868671, 0.002036241814494133, 0.017864735797047615, 0.0016671631019562483, 0.029823707416653633, 0.04792991653084755, 0.0008173534879460931, 0.0003271626483183354, 0.03921588137745857, 0.0014942979905754328, 0.0041770124807953835, 0.001864289864897728, 0.0033449316397309303, 0.0013050211127847433, 0.0005733717116527259, 0.0007127019925974309, 0.008124278858304024, 0.01756216585636139, 0.0015016711549833417, 0.00035910907899960876, 0.0045983074232935905, 0.004402314778417349, 0.002325433772057295, 0.7375267148017883, 0.045573022216558456, 0.0064311460591852665, 0.0159740149974823, 0.0003671428421512246, 0.006013731472194195, 0.04251130297780037, 3.648626079666428e-05, 0.003994252532720566, 9.418460103916004e-05, 0.023529520258307457, 0.00011627382627921179, 0.00010813562403200194, 0.05905766785144806, 0.009341850876808167, 0.008406956680119038, 0.0007185578579083085, 0.00014937833475414664, 0.0068934266455471516, 0.003695478430017829, 0.0008696955046616495, 0.00644516758620739, 0.001889537670649588, 0.0003708720614667982, 0.02455725707113743, 0.006081210449337959, 0.011334612034261227, 0.0072408621199429035, 0.0018406519666314125, 0.0005462176050059497, 0.0021614928264170885, 0.0021483644377440214, 0.013470044359564781, 0.053000494837760925, 0.013240865431725979, 0.004297418519854546, 0.03978902846574783, 0.003845247905701399, 0.006728924345225096, 0.00015413334767799824, 0.0007587745203636587, 7.477909093722701e-05, 0.0026620873250067234, 0.0017122493591159582, 0.00042742298683151603, 0.008478136733174324, 0.02927173301577568, 0.005777936894446611, 0.004578851629048586, 0.0004681671562138945, 0.0004381464095786214, 0.01911466009914875, 0.0015363737475126982, 0.0012794554932042956, 0.002847474068403244, 0.0012915135594084859, 2.4456468963762745e-05, 0.004622678738087416, 0.0020469685550779104, 0.00011658056610031053, 0.012804555706679821, 0.0031766225583851337, 0.004113397095352411, 0.06707973778247833, 0.0003062103933189064, 0.004247831180691719, 0.028920968994498253, 0.005993692670017481, 0.1279142051935196, 0.005378806963562965, 0.0006261375383473933, 0.024316102266311646, 0.018878981471061707, 0.01569284126162529, 0.003352544968947768, 0.062299273908138275, 0.014220453798770905, 2.6757898012874648e-05, 0.01882742904126644, 0.0005284672952257097, 0.032599661499261856, 0.0741053894162178, 0.003508841386064887, 2.4398768800892867e-05, 0.00022050415282137692, 0.013601034879684448, 0.0005376521730795503, 0.01333676278591156, 0.0037777922116219997, 0.005583413876593113, 0.04602459818124771, 0.00804708432406187, 0.10889487713575363, 0.0013355826959013939, 9.588035027263686e-05, 0.004205864854156971, 0.0008931838674470782, 0.008838695473968983, 0.00016934340237639844, 0.00010979444778058678, 0.004621768370270729, 0.016332020983099937, 0.014666279777884483, 0.0003263785329181701, 0.000559703737962991, 0.20722229778766632, 0.0014690455282106996, 0.0004970725276507437, 0.044136155396699905, 0.0009815869852900505, 0.007708475459367037, 0.7820163369178772, 0.001914139254949987, 0.0008024704293347895, 0.003000252414494753, 0.004831789527088404, 0.05770031362771988, 0.01791202649474144, 0.002407347084954381, 0.004862356930971146, 0.00020324939396232367, 0.009112031199038029, 0.005984339863061905, 0.0006821426213718951, 0.006628218572586775, 0.21268992125988007, 0.0035957766231149435, 0.000995886279270053, 0.0047059799544513226, 0.00011105368321295828, 0.0007012622663751245, 0.0015098260482773185, 0.004331653937697411, 0.006239047273993492, 0.007200255058705807, 0.01866242289543152, 0.0030638333410024643, 0.0004355753189884126, 0.015363689512014389, 0.027405884116888046, 0.00623175548389554, 0.0008252311963587999, 0.004465443082153797, 0.001270526205189526, 0.004572316538542509, 0.035290613770484924, 0.017251433804631233, 0.8043448328971863, 0.026887640357017517, 0.0005264504579827189, 0.0014684965135529637, 9.684906399343163e-05, 0.004305925220251083, 0.0012899202993139625, 0.14125767350196838, 0.0065297274850308895, 0.04656703397631645, 0.005741330329328775, 0.00456901453435421, 0.006960713304579258, 0.018417324870824814, 0.00047728753997944295, 0.004579029977321625, 0.0016703533474355936, 0.002910757903009653, 0.021107302978634834, 0.003936152905225754, 0.12929590046405792, 0.014698522165417671, 0.0003135718870908022, 0.0008378600468859076, 0.020938623696565628, 0.0015925203915685415, 0.000573902390897274, 0.03606557846069336, 0.003580766962841153, 0.004962814971804619, 0.013710585422813892, 0.0007720282301306725, 0.011309044435620308, 0.04154511168599129, 0.0017188393976539373, 0.007409337908029556, 0.007065902464091778, 0.008570598438382149, 0.004796427674591541, 0.0004503750242292881, 0.0019164399709552526, 0.0048165591433644295, 0.001987352268770337, 0.003219241974875331, 0.001944966148585081, 0.04870731011033058, 0.016896234825253487, 0.003971964120864868, 0.05349769443273544, 0.00955151952803135, 0.017276916652917862, 0.010138059966266155, 0.0010326416231691837, 0.00615805946290493, 0.011140388436615467, 0.054681405425071716, 0.0003011510707437992, 0.00012922023597639054, 0.005227378569543362, 0.17128224670886993, 0.002782566472887993, 0.0007455055601894855, 0.0016303850570693612, 0.003172866767272353, 0.00010167559230467305, 0.006775473244488239, 0.0012045049807056785, 0.04058986157178879, 0.0026741172187030315, 0.019432591274380684, 0.0002121052675647661, 0.00013849171227775514, 0.008036649785935879, 0.0002957576944027096, 0.010784282349050045, 0.0007746292394585907, 0.00020644866162911057, 0.001607390120625496, 0.020141780376434326, 0.002530462574213743, 4.3623072997434065e-05, 0.0018166103400290012, 0.0010682365391403437, 0.0008112809155136347, 0.005496171768754721, 0.003516554133966565, 0.1523379683494568, 0.05275346711277962, 0.010430688969790936, 0.0008918546955101192, 0.0008890950703062117, 0.005346505902707577, 0.0005749768461100757, 0.0018393725622445345, 0.004601732827723026, 0.0017904292326420546, 0.0019108178094029427, 0.025448285043239594, 0.09976977854967117, 0.032863616943359375, 0.0002880577521864325, 0.04594428092241287, 0.008111738599836826, 0.0008432093891315162, 0.00042687877430580556, 0.036485426127910614, 5.30703691765666e-05, 0.0038185168523341417, 4.617831655195914e-05, 0.00478791818022728, 0.001536417636089027, 0.015488168224692345, 0.002183619188144803, 0.023044003173708916, 0.007370080333203077, 0.04523936286568642, 0.014425531961023808, 0.002518060151487589, 0.058195147663354874, 0.14552171528339386, 0.0975470095872879, 0.0034546477254480124, 0.006428272929042578, 0.02857387065887451, 0.0006516733556054533, 0.010837125591933727, 0.000522174290381372, 0.0010054465383291245, 0.010947478003799915, 0.017788169905543327, 0.005680016707628965, 0.005969553720206022, 0.002691650064662099, 3.1515921818936476e-06, 0.0063586668111383915, 0.0006139556062407792, 0.0006420289282687008, 0.003977500833570957, 0.03268526867032051, 0.0008403434767387807, 0.005762180313467979, 0.007611374836415052, 0.002168654464185238, 0.0021375715732574463, 0.01136097963899374, 0.0017106948653236032, 0.00729253189638257, 0.04406014457345009, 0.00041971090831793845, 0.046478480100631714, 0.010297083295881748, 0.0003995304286945611, 0.002342845778912306, 7.066850957926363e-05, 0.003492048941552639, 8.420840458711609e-05, 0.0008420154335908592, 0.0007617555093020201, 0.013238423503935337, 0.0017665908671915531, 0.0014327768003568053, 0.006804192438721657, 0.0019325506873428822, 0.0069504366256296635, 0.00018960240413434803, 0.0022068647667765617, 0.00423806207254529, 0.0007237641839310527, 0.0022536618635058403, 0.018941406160593033, 0.023361049592494965, 0.008869384415447712, 0.0032729855738580227, 0.0020944641437381506, 0.16557033360004425, 0.00392498541623354, 0.00016121004591695964, 0.008604273200035095, 0.0009105998906306922, 0.07721627503633499, 0.0018548488151282072, 0.0064963772892951965, 0.05884833633899689, 0.0029796482995152473, 0.005600521340966225, 0.0005314846057444811, 0.04997606575489044, 0.0002204541233368218, 0.001012397464364767, 0.0008382034138776362, 0.0016232506604865193, 0.0022812755778431892, 0.0009662225493229926, 0.0009098306763917208, 0.0045351991429924965, 0.00024204421788454056, 0.00016494272858835757, 0.001724611734971404, 0.0022393111139535904, 0.0009064612095244229, 0.00016153563046827912, 0.07355988025665283, 0.0010382363107055426, 0.007577488198876381, 0.0009273571195080876, 0.0064741638489067554, 0.007147954776883125, 0.0013250766787678003, 0.0004642984422389418, 2.352138835703954e-06, 0.003520167199894786, 0.02468419261276722, 0.01233583316206932, 5.5360436817863956e-05, 0.0012089054798707366, 0.008015185594558716, 0.003322141943499446, 0.00399215891957283, 0.005243796389549971, 9.842679719440639e-05, 0.012737948447465897, 0.0031496689189225435, 0.008665276691317558, 0.0024206016678363085, 0.01611142046749592, 0.0005001194076612592, 0.0055722626857459545, 0.0018305199919268489, 0.0002797761408146471, 0.02938310243189335, 0.0002944536099676043, 0.001694065984338522, 0.004852977581322193, 0.39156094193458557, 0.02792779915034771, 0.00222638132981956, 0.0003610814455896616, 0.0014821592485532165, 0.001421382767148316, 0.0006797544774599373, 0.00011474600614747033, 0.023771626874804497, 0.000986658502370119, 0.017666926607489586, 0.0030347679276019335, 0.0003609422128647566, 0.025388598442077637, 0.00018943578470498323, 0.008274140767753124, 0.002718945499509573, 0.017518796026706696, 0.0016651737969368696, 7.248864130815491e-05, 0.00916352029889822, 0.0003404540184419602, 0.0006250261212699115, 0.0059981378726661205, 0.0013258473481982946, 0.030018839985132217, 0.0006822775467298925, 0.02893931418657303, 0.0018613693537190557, 0.01994198001921177, 0.0011838383506983519, 0.0034652294125407934, 0.026667671278119087, 0.028710927814245224, 0.0036768752615898848, 0.0010439353063702583, 0.0024639314506202936, 0.0018182704225182533, 0.015949726104736328, 0.0030650219414383173, 0.0019755889661610126, 0.0006491439417004585, 0.0008093832875601947, 0.006311705335974693, 0.0024230151902884245, 0.008075295016169548, 0.00012326415162533522, 0.0023605499882251024, 0.034613076597452164, 0.0011785171227529645, 0.0025184445548802614, 0.028888298198580742, 0.11720017343759537, 0.007057436741888523, 0.0011143055744469166, 0.0012473874958232045, 0.010606651194393635, 0.0036030691117048264, 0.016580048948526382, 0.05192387104034424, 0.06415598839521408, 0.0006154440343379974, 0.0032557642553001642, 0.009391991421580315, 0.0005513255600817502, 0.002735432703047991, 0.012456837110221386, 0.08689628541469574, 0.005113685503602028, 0.005858807824552059, 8.193736721295863e-05, 0.0029650297947227955, 0.0028366849292069674, 0.03486374020576477, 0.0024840496480464935, 0.1577402651309967, 0.000692450616043061, 1.603826785867568e-05, 0.06443631649017334, 0.0012047693599015474, 0.01197090744972229, 0.09567581862211227, 0.0009970616083592176, 0.003729436080902815, 0.04033242538571358, 0.007825272157788277, 0.0010670857736840844, 0.0012947008945047855, 0.005644065327942371, 0.03162530064582825, 0.003872743109241128, 0.0019574174657464027, 0.0023507613223046064, 0.001862604753114283, 0.0001092905513360165, 0.0020749082323163748, 0.024874845519661903, 0.13226710259914398, 0.0033771228045225143, 0.047254566103219986, 0.00010489878332009539, 0.0009448666241951287, 0.0009251327719539404, 0.0008113366202451289, 0.0035080176312476397, 0.000617369485553354, 0.0012761573307216167, 0.002819968620315194, 0.05713570863008499, 0.1563291996717453, 0.0268213152885437, 0.0034462776966392994, 0.016895482316613197, 4.3658448703354225e-05, 0.006338504143059254, 0.00027095279074274004, 0.010340939275920391, 0.01012328453361988, 0.047265034168958664, 0.003916886169463396, 0.022222256287932396, 0.012584409676492214, 0.0030843738932162523, 0.002370926085859537, 0.006788311526179314, 0.000273185403784737, 0.018196115270256996, 0.03328483924269676, 0.00043940040632151067, 0.006515556946396828, 0.009118275716900826, 0.02371929958462715, 0.0021809255704283714, 0.008940096944570541, 0.0011151906801387668, 0.0028854196425527334, 6.935649139450106e-07, 0.10981874167919159, 0.025241034105420113, 0.000472215935587883, 0.04136847332119942, 0.0188571996986866, 0.005303089506924152, 0.01603008061647415, 3.0097184207988903e-05, 0.0037183668464422226, 0.373313844203949, 0.0027229655534029007, 0.0016306800534948707, 0.0182986781001091, 0.0021315384656190872, 0.005104160401970148, 0.0007123957620933652, 0.003359233494848013, 0.0007084837416186929, 0.006079951301217079, 0.0015440528513863683, 0.0008583544404245913, 0.0015440431889146566, 0.003292104694992304, 0.002566303825005889, 0.0005383872776292264, 3.88226326322183e-05, 6.8555527832359076e-06, 0.004758999217301607, 0.0008860277594067156, 0.0032069829758256674, 0.0003423366870265454, 0.0008515846566297114, 0.0025778640992939472, 0.0034037234727293253, 0.0031674359925091267, 0.00038127557490952313, 0.0001798440789571032, 8.677872392581776e-05, 0.0021048756316304207, 0.0023081901017576456, 0.005500362254679203, 0.02076716348528862, 0.004089310299605131, 0.02824299782514572, 0.06631328910589218, 0.0027926922775804996, 0.0009127655066549778, 0.013328170403838158, 0.0012162866769358516, 0.0009302030666731298, 0.0116092124953866, 0.019023699685931206, 0.02952607348561287, 0.0007266246248036623, 0.03427659720182419, 0.006607838440686464, 0.0007607406005263329, 0.058723460882902145, 0.0022341785952448845, 0.03944773226976395, 0.022282959893345833, 0.0012780785327777267, 0.00047958080540411174, 0.02280741184949875, 0.012917784973978996, 0.013592190109193325, 0.00016969820717349648, 0.002777870511636138, 0.005063790362328291, 0.00038888122071512043, 0.001884706667624414, 0.00799145270138979, 1.964542389032431e-05, 0.0034637197386473417, 0.033944498747587204, 0.00020137497631367296, 0.00579384621232748, 0.00035864513483829796, 0.0037646929267793894, 0.0009428297635167837, 0.000568390591070056, 0.015080488286912441, 0.0019882426131516695, 0.003271915717050433, 0.00012573708954732865, 0.06061835214495659, 0.0001149760209955275, 0.021829238161444664, 4.917413752991706e-05, 0.00014624248433392495, 0.008461526595056057, 0.0008985705790109932, 0.00147377944085747, 0.0003223636595066637, 0.046725839376449585, 0.9943864345550537, 0.022168349474668503, 0.004897272679954767, 0.00023369802511297166, 0.004841270390897989, 0.004391544032841921, 0.0032196182291954756, 0.004972393624484539, 0.005645253229886293, 0.00065996014745906, 0.007192634046077728, 0.0010095566976815462, 0.0004231739731039852, 7.953273598104715e-05, 0.001445479691028595, 0.0028480524197220802, 0.004838948603719473, 0.018861522898077965, 0.0064600794576108456, 0.008844600059092045, 0.006216865964233875, 0.001814975868910551, 0.018726656213402748, 0.00016840174794197083, 0.10024915635585785, 0.0015033645322546363, 0.00303051620721817, 4.898883344139904e-05, 0.0006605321541428566, 0.0020548540633171797, 0.00095975655131042, 0.001076884800568223, 0.005097709130495787, 0.0002881225664168596, 0.01668616756796837, 0.004894993733614683, 0.0025570187717676163, 0.01947813294827938, 0.0019329194910824299, 0.000647225824650377, 0.002101215301081538, 0.03793693706393242, 0.0003966859949287027, 0.003342795418575406, 0.0021791094914078712, 0.07966823875904083, 0.005272185895591974, 0.012772101908922195, 0.0014779908815398812, 0.0004946890403516591, 0.004037398844957352, 0.022751668468117714, 0.006185639649629593, 0.006609143689274788, 0.00037598228664137423, 0.0003092902188654989, 9.62374106165953e-05, 0.01822296716272831, 0.0043513718992471695, 0.0006246893317438662, 0.0016481595812365413, 0.004864189773797989, 0.002661727601662278, 0.004884594120085239, 0.0001254807721124962, 0.006963940337300301, 0.02876182086765766, 0.005794856697320938, 0.0008744135266169906, 0.00040395453106611967, 0.06687653064727783, 0.003891266882419586, 0.0017371071735396981, 0.034313369542360306, 0.04199967160820961, 0.06356147676706314, 0.0019955490715801716, 0.0004867989046033472, 0.02831176668405533, 0.03615798428654671, 0.007864225655794144, 0.00038107152795419097, 0.0027402108535170555, 0.9247270822525024, 0.026488235220313072, 3.3025102311512455e-05, 0.002911337884142995, 0.0017799768829718232, 0.026864899322390556, 0.010914159938693047, 0.0011272147530689836, 0.0026177221443504095, 0.0010560000082477927, 0.0014722946798428893, 0.05849526822566986, 0.0024925770703703165, 0.0013600094243884087, 0.03223518282175064, 0.00023413894814439118, 0.008884659968316555, 0.006391887087374926, 0.0018190910341218114, 0.0033485149033367634, 0.05216564983129501, 0.013561817817389965, 0.0023991877678781748, 3.5914996260544285e-05, 0.001556868082843721, 0.003597988048568368, 0.00014573258522432297, 0.0017515901708975434, 0.013626808300614357, 0.0034535794984549284, 0.0013977860799059272, 0.0021051992662250996, 0.011374657042324543, 0.00965393241494894, 0.0008832524181343615, 0.0005710709956474602, 0.0029479602817445993, 0.0017466871067881584, 0.02213129960000515, 0.0003597316681407392, 0.00667702266946435, 0.0020932545885443687, 0.005625477526336908, 0.00514170853421092, 0.004186667036265135, 0.0029891617596149445, 0.013344340026378632, 0.061365365982055664, 0.08818243443965912, 7.857958735257853e-06, 0.017597248777747154, 0.0009822971187531948, 0.004122018348425627, 0.04845559969544411, 0.0333065427839756, 0.23623448610305786, 0.0021794061176478863, 3.523463965393603e-05, 0.00025135328178294003, 0.03608057647943497, 0.140666663646698, 0.00022490603441838175, 0.04703124612569809, 0.007174357771873474, 0.00717706186696887, 0.004399041645228863, 0.08283097296953201, 0.0029412643052637577, 0.00038015987956896424, 0.05325530096888542, 0.06286889314651489, 0.007509415503591299, 0.042673684656620026, 0.030867023393511772, 0.0007387142977677286, 0.002239364432170987, 0.005503411870449781, 4.414816794451326e-05, 0.0023493615444749594, 0.05213429406285286, 0.0007070157444104552, 0.0006070371018722653, 0.008476979099214077, 0.0011605037143453956, 0.004470637533813715, 0.0015404380392283201, 7.112374532880494e-06, 0.0010389437666162848, 0.0013385956408455968, 0.003803059458732605, 0.001718795276246965, 2.691829649847932e-05, 0.00020745143410749733, 0.00024071561347227544, 0.004026868846267462, 0.002443418139591813, 0.007602839730679989, 0.0006497595459222794, 0.0023108699824661016, 0.003409889992326498, 0.001480786595493555, 0.001049723126925528, 0.0031890736427158117, 0.06552528589963913, 0.0038812067359685898, 0.0006500395247712731, 0.0022270139306783676, 3.2610118068987504e-05, 0.004021391272544861, 0.0012543805642053485, 0.0008688469533808529, 0.011127976700663567, 4.866633753408678e-05, 0.002857036655768752, 0.00027541062445379794, 0.001251613604836166, 0.0017963277641683817, 0.0069735366851091385, 0.0534542016685009, 0.01942855305969715, 0.0007721750298514962, 0.0008928243769332767, 1.5545485894108424e-06, 0.004111867863684893, 0.0004868137766607106, 0.010626601055264473, 0.0005563043523579836, 0.005294456612318754, 0.010352316312491894, 0.0013403644552454352, 0.0033599999733269215, 0.02814630977809429, 0.011071533896028996, 0.07042807340621948, 0.004138530697673559, 0.00021879313862882555, 0.032582372426986694, 0.005208609625697136, 0.0002769988204818219, 0.008521360345184803, 0.0003574821457732469, 0.0001777978613972664, 0.0024437482934445143, 0.004736566916108131, 0.0010792278917506337, 0.0006053278339095414, 0.006486099679023027, 0.009618278592824936, 0.04212186485528946, 0.0002397133648628369, 0.0018173444550484419, 0.004527254030108452, 0.07407338172197342, 0.0040488215163350105, 0.001976153114810586, 0.0001280376163776964, 0.00013471835700329393, 0.07514297962188721, 0.006401545833796263, 0.0012991632102057338, 0.00552667211741209, 7.5433352321852e-05, 0.0017796293832361698, 0.048865411430597305, 0.010928386822342873, 0.008817674592137337, 0.002695422386750579, 0.0024435229133814573, 0.001550657325424254, 0.0015161948977038264, 0.0026838378980755806, 0.0038532710168510675, 0.026232343167066574, 0.008580361492931843, 0.00044619583059102297, 0.0034338340628892183, 0.01605057343840599, 0.012084726244211197, 0.13309495151042938, 0.018420778214931488, 0.002076113363727927, 0.00019986781990155578, 0.019416315481066704, 0.06266926974058151, 0.00013681178097613156, 0.0070248250849545, 0.06217647343873978, 0.04075157642364502, 0.001209403621032834, 0.006007993593811989, 0.012494858354330063, 0.00038436424802057445, 0.0019389025401324034, 0.0051745702512562275, 0.04265926033258438, 0.005187748931348324, 0.0019869704265147448, 0.00021128336084075272, 0.003599839983507991, 0.006447693333029747, 0.011515805497765541, 0.0026924593839794397, 0.0005397159839048982, 0.03427254036068916, 0.0026343874633312225, 0.00037385994801297784, 0.003930830396711826, 0.004395294468849897, 0.008734622970223427, 0.0017544181318953633, 0.0004184295248705894, 0.05428784340620041, 0.00653222156688571, 0.0010490053100511432, 0.0025112424045801163, 0.0026433393359184265, 0.0028703801799565554, 0.0001694740931270644, 0.03504404053092003, 0.028106892481446266, 0.011406820267438889, 0.0033951792865991592, 0.012845789082348347, 0.043314285576343536, 0.0034222567919641733, 0.007826101034879684, 0.0014631260419264436, 0.009074731729924679, 0.00032324937637895346, 0.001422119210474193, 0.001873427419923246, 9.55462091951631e-05, 0.00299640791490674, 0.001929537975229323, 0.001617662375792861, 0.004807347431778908, 0.0003925692872144282, 0.001729471143335104, 0.0005556282121688128, 0.0009081833413802087, 0.0001413231948390603, 0.004628967028111219, 0.00147493917029351, 0.002177664777263999, 0.005335463676601648, 0.011189558543264866, 0.0021159439347684383, 0.0035704600159078836, 0.0010596481151878834, 0.0007390924147330225, 0.003228263696655631, 0.09930810332298279, 0.0021576406434178352, 0.002670788671821356, 0.04672155901789665, 0.0013618330704048276, 0.00015986191283445805, 0.04911207780241966, 6.363460124703124e-05, 0.0003176762256771326, 0.0011136089451611042, 0.0022193146869540215, 0.0019359546713531017, 0.004545330535620451, 0.001962617738172412, 0.09113568067550659, 0.00038741223397664726, 0.0009473604150116444, 0.009274766780436039, 0.007168901618570089, 0.0049865455366671085, 0.0033120494335889816, 0.00021543075854424387, 0.00022865270148031414, 0.6304753422737122, 0.07174620777368546, 0.03907400369644165, 5.634738408843987e-05, 0.000657467870041728, 0.008556344546377659, 0.014113574288785458, 0.0005617678398266435, 0.0048293485306203365, 0.000423544057412073, 0.09464061260223389, 0.017457257956266403, 0.005176946986466646, 0.0014963612193241715, 0.0489424392580986, 0.0007823603809811175, 0.0424954779446125, 0.0017920450773090124, 0.07136648893356323, 0.0042476290836930275, 0.10381744056940079, 0.008028876036405563, 0.0010983631946146488, 0.004860214423388243, 0.002382827689871192, 0.9938680529594421, 0.0006481640739366412, 0.00019070785492658615, 0.005095141474157572, 0.030575746670365334, 0.015742547810077667, 0.0003293633053544909, 0.029942860826849937, 2.847201540134847e-05, 0.00821093562990427, 0.0123402439057827, 0.009379502385854721, 0.0007384939817711711, 0.0012597140157595277, 2.5212671971530654e-05, 0.0015970333479344845, 0.003441222244873643, 0.0005842079408466816, 0.0012177692260593176, 0.001708958763629198, 0.0022205777931958437, 0.00831025093793869, 0.0012406576424837112, 0.008426298387348652, 0.001128266449086368, 0.05226798728108406, 0.04932457581162453, 0.00020226034393999726, 6.749061139998958e-05, 0.0013834513956680894, 0.04564541578292847, 0.01115815807133913, 0.004126228857785463, 0.0061356988735497, 0.0030041802674531937, 9.737115760799497e-05, 0.014465190470218658, 0.004749381449073553, 0.022173110395669937, 0.002698979340493679, 0.038050394505262375, 0.06761237233877182, 0.002471053507179022, 0.00833500549197197, 0.0031070730183273554, 0.002436081413179636, 0.13149815797805786, 0.0005920936819165945, 0.005603895056992769, 0.004342223517596722, 0.004960950464010239, 0.028299391269683838, 0.003910753410309553, 0.002076073782518506, 0.00011110028572147712, 0.006755149457603693, 0.026758091524243355, 5.849228546139784e-05, 0.00014337452012114227, 0.011526226066052914, 0.002137451432645321, 0.0016307149780914187, 0.21231085062026978, 0.0007939322385936975, 0.0031146707478910685, 0.05439542979001999, 0.0051195258274674416, 0.0015329929301515222, 0.02468736469745636, 0.009083096869289875, 0.02225504070520401, 0.0028965480159968138, 0.0012985268840566278, 0.0026460234075784683, 0.0011624459875747561, 0.004511098377406597, 0.0023639772552996874, 0.01128455065190792, 0.011505724862217903, 5.586009501712397e-05, 0.0001759357110131532, 0.05549192801117897, 0.003383273258805275, 0.01309371180832386, 0.004547842778265476, 0.00013669455074705184, 0.001130403601564467, 0.005713958293199539, 0.004018546547740698, 0.0011364287929609418, 0.0013322109589353204, 0.0013207902666181326, 0.0012930366210639477, 0.00373417092487216, 0.01492503471672535, 0.0014569438062608242, 0.0075032408349215984, 0.004819384776055813, 2.8722150091198273e-05, 0.003353383159264922, 0.000945483916439116, 0.010034376755356789, 0.0011911545880138874, 0.12423592060804367, 0.0038140390533953905, 0.05255604162812233, 0.002326968591660261, 0.0004491005092859268, 0.003040171228349209, 0.02756187878549099, 0.043974146246910095, 0.00040293633355759084, 0.005187450908124447, 0.008917812258005142, 0.02510800026357174, 0.003625617828220129, 0.002625047694891691, 0.0003053675754927099, 0.03658753260970116, 0.0015484964242205024, 0.004706431180238724, 0.0047857691533863544, 0.06140507385134697, 0.025560297071933746, 0.011086276732385159, 0.14484632015228271, 0.2090878039598465, 0.003931437153369188, 0.2518377900123596, 0.0001998575171455741, 0.013413222506642342, 0.0005307652172632515, 0.0004791252431459725, 0.00219548586755991, 0.0010350823868066072, 0.0012955600395798683, 0.002606272231787443, 0.022570021450519562, 0.0020071924664080143, 0.004959343001246452, 0.0036788848228752613, 0.005931940861046314, 0.004307984374463558, 0.0017516528023406863, 0.004220013972371817, 0.008997024036943913, 0.0049041020683944225, 0.002126879058778286, 0.027205880731344223, 0.04039819911122322, 0.015255174599587917, 0.0019311320502310991, 0.004967566579580307, 0.0017464252887293696, 0.0035287849605083466, 0.0002922271960414946, 0.010778184980154037, 0.00010179152013733983, 0.0001751015952322632, 0.004167799372226, 0.049079813063144684, 2.3427843188983388e-05, 0.004177375230938196, 0.005620609037578106, 0.006954025477170944, 0.0003496738791000098, 0.0011067272862419486, 0.002415614202618599, 0.028288831934332848, 0.006150615867227316, 0.005936449859291315, 0.0020622056908905506, 0.00612955167889595, 0.00042062491411343217, 0.0033505859319120646, 0.00012635246093850583, 0.00025194374029524624, 0.001619404531084001, 0.0019774918910115957, 0.010602039285004139, 0.020511778071522713, 0.0028171848971396685, 0.0012395798694342375, 0.002395011018961668, 0.020391762256622314, 0.779081404209137, 0.006229581777006388, 0.0024220068007707596, 0.00596732459962368, 0.00020812306320294738, 0.057963624596595764, 0.004653468728065491, 0.0037608500570058823, 0.004980452358722687, 0.00570070743560791, 0.0014869925798848271, 0.0036154913250356913, 0.0010147086577489972, 0.0013715304667130113, 0.003193141659721732, 0.024072270840406418, 0.0043223705142736435, 0.18064500391483307, 0.0067657288163900375, 0.01765529438853264, 0.011354533955454826, 0.00010418560850666836, 0.02959730289876461, 0.0003403154551051557, 0.0003268808068241924, 0.004651706665754318, 0.0026412219740450382, 0.0033588840160518885, 0.0009035530965775251, 0.0018422892317175865, 0.0036956227850168943, 9.680326911620796e-05, 0.26716452836990356, 0.0016202597180381417, 0.001856970600783825, 0.0005236356519162655, 0.03250958397984505, 0.10899630188941956, 0.017981788143515587, 0.0007251359638758004, 0.0018695357721298933, 0.001863811630755663, 0.004326373804360628, 0.03817683830857277, 0.01844146102666855, 0.009207834489643574, 0.015199980698525906, 0.004047489259392023, 0.005262819118797779, 0.0003796962555497885, 0.004247048404067755, 0.004441363736987114, 0.013435186818242073, 9.546342334942892e-05, 0.03337358310818672, 0.02170468680560589, 0.005867090076208115, 0.01317193079739809, 0.0050863358192145824, 0.009080037474632263, 0.001444482710212469, 0.01151603925973177, 0.013714584521949291, 0.0017938336823135614, 0.1828136444091797, 0.001770887873135507, 0.008269630372524261, 0.0025755981914699078, 0.0018854503287002444, 0.05570721998810768, 0.01213905867189169, 0.0006365235894918442, 2.984199090860784e-05, 0.0010165394051000476, 0.07626143097877502, 0.0018213964067399502, 0.002408009022474289, 0.00561766792088747, 0.00045926281018182635, 0.0008064777939580381, 0.011825433932244778, 0.002958203898742795, 0.0042212060652673244, 0.02453436516225338, 0.0007500837673433125, 0.0025057790335267782, 0.00015984271885827184, 0.0010243551805615425, 0.0005926786107011139, 0.0029334325809031725, 0.019316354766488075, 0.0069534024223685265, 0.0002671114925760776, 0.002822191221639514, 0.05257744342088699, 0.04702065512537956, 0.0015975473215803504, 0.001552658504806459, 0.0010950659634545445, 0.019039960578083992, 0.011028450913727283, 0.0011498744133859873, 0.0039036348462104797, 0.010793422348797321, 0.017360828816890717, 0.0004230287449900061, 0.0009263799292966723, 0.00140233407728374, 0.001914581866003573, 0.003809022018685937, 0.027496250346302986, 2.697362106118817e-05, 0.0005476731457747519, 0.004832677077502012, 0.006872178986668587, 0.00061547860968858, 0.0034388210624456406, 0.07683733850717545, 0.02470262348651886, 0.039490386843681335, 0.0018743861000984907, 0.016086963936686516, 0.01430579088628292, 0.0024293221067637205, 0.15209858119487762, 0.0003445120819378644, 0.016002628952264786, 0.0028139075729995966, 0.004044212866574526, 0.0009121321490965784, 2.3025822883937508e-05, 5.386885823099874e-05, 0.0017055259086191654, 0.02063089981675148, 0.005022710654884577, 0.035945288836956024, 0.02014489658176899, 9.10772942006588e-05, 0.014064572751522064, 0.002171254251152277, 0.002828463912010193, 0.0002922274870797992, 0.004655490163713694, 0.0036355999764055014, 0.0001026719983201474, 0.020510653033852577, 0.0013008428504690528, 0.0006287351716309786, 0.0009208859992213547]\n",
      "Decoded: [2.733194014581386e-05, 9.75596663010947e-07, 6.477948045358062e-05, 6.041648703103419e-06, 2.2891547359904507e-06, 7.419065696012694e-06, 2.3255449832504382e-06, 0.00010839031165232882, 7.448587712133303e-05, 0.0014122118009254336, 0.0007299344870261848, 0.0025617601349949837, 0.0011476000072434545, 0.002111355774104595, 0.003562136320397258, 0.005291682668030262, 0.00788692757487297, 0.01729116402566433, 0.010949363000690937, 0.027163894847035408, 0.011273851618170738, 0.01577664352953434, 0.0197629164904356, 0.008961626328527927, 0.028107719495892525, 0.02992722950875759, 0.16499826312065125, 0.07600721716880798, 0.20017550885677338, 0.14596088230609894, 0.23676545917987823, 0.3266671895980835, 0.23850779235363007, 0.3010807931423187, 0.32215556502342224, 0.3560752868652344, 0.36647510528564453, 0.277660071849823, 0.3836795687675476, 0.16125409305095673, 0.3826339542865753, 0.24295161664485931, 0.39283812046051025, 0.25977185368537903, 0.3173917233943939, 0.3221207857131958, 0.31198832392692566, 0.3960590362548828, 0.2741956114768982, 0.3090285658836365]\n",
      "Texto: !!!!!!!!!\n",
      "\"! -,!!!!!!!19!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# Separar encoder y decoder\n",
    "encoder, decoder = split_model(model)\n",
    "#\n",
    "# Datos de prueba\n",
    "#\n",
    "sample_text = \"¿De qué manera puedo probar esto?\"\n",
    "tokenizado = []\n",
    "for ttkn, n_grama in splitter.split_by_type(sample_text, 7):\n",
    "    valores = splitter.normalize_n_gram(n_grama)\n",
    "    if len(valores) > 1:\n",
    "        tokenizado.append(K.MAIN_TOKEN_DICT[valores[0]])\n",
    "    tokenizado.extend(tokeniza(valores[-1], ttkn))\n",
    "input_raw = [token_id_real[x] for x in tokenizado]\n",
    "X_test = [[0.0]*(NUM_CONTEXT_TOKENS - len(input_raw)) + input_raw]\n",
    "print(len(X_test[0]), detokeniza(X_test[0]))\n",
    "sample_input = torch.tensor(X_test, dtype=torch.float32)\n",
    "#\n",
    "# Inferencia\n",
    "#\n",
    "encoded_features = encoder(sample_input)\n",
    "decoded_output = decoder(encoded_features)\n",
    "#\n",
    "# Resultado\n",
    "#\n",
    "print(\"Input:\", sample_input[0].tolist())\n",
    "print(\"Encoded (embeddings):\", encoded_features[0].tolist())\n",
    "print(\"Decoded:\", decoded_output[0].tolist())\n",
    "print(\"Texto:\", detokeniza([x for x in decoded_output[0].tolist() if x >= 0.0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
