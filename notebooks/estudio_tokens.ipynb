{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:grey;\"> Tokenizador de textos </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Memoria: svmem(total=12649299968, available=6288945152, percent=50.3, used=6360354816, free=6288945152)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import psutil\n",
    "import pickle\n",
    "from math import log\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dfrtokenuniverse.splitter\n",
    "import dfrtokenuniverse.word_inventory\n",
    "import dfrtokenuniverse.constantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recarga librerías propias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dfrtokenuniverse.splitter)\n",
    "importlib.reload(dfrtokenuniverse.word_inventory)\n",
    "importlib.reload(dfrtokenuniverse.constantes)\n",
    "from dfrtokenuniverse.splitter import TextSplitter\n",
    "from dfrtokenuniverse.constantes import KDfrNlp\n",
    "K = KDfrNlp()\n",
    "splitter = TextSplitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diccionario de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo D:\\datos\\tokens_ttkn_id_v20241208.pkl\n",
      "Nada más leer:\n",
      "6 Del Sistema 9 ['<', '|', 'v', 'a', 'i', 'd', '2', '|', '>']\n",
      "1 Letras 14137 ['bar', 'bom', 'bo', 'deo', 'eo', 'de', 'ser', 'ivk', 'el', 'rr']\n",
      "2 Separadores 359 [' , ', ' ,', '  ', '. ', ', ', '   ', ' .', ' . ', ' \"', '\".']\n",
      "0 Dígitos 920 ['22', '20', '01', '00', '21', '12', '16', '38', '020', '521']\n",
      "3 Símbolos 227 ['}}', '((', '))', '])', '>>', '}{', '})', ')}', ']}', '<{']\n",
      "5 Desconocidos 5744 ['ɾ', 'ˈ', 'ː', 'ž', 'č', '\\u200b', '«', '»', '“', '”']\n",
      "4 Especiales 8 ['\\t', '\\t\\t', '\\t\\t\\t', '\\x08', '\\x07', '\\x0c', '\\r', '\\x0b']\n",
      "Arreglado:\n",
      "6 Del Sistema 10 ['<|PAD|>', '<|UPPER|>', '<|LOWER|>', '<|CAP|>', '<|START|>', '<|TBC|>', '<|EOF|>', '<|UNK|>', '<|void1|>', '<|vaid2|>']\n",
      "1 Letras 14137 ['bar', 'bom', 'bo', 'deo', 'eo', 'de', 'ser', 'ivk', 'el', 'rr']\n",
      "2 Separadores 359 [' , ', ' ,', '  ', '. ', ', ', '   ', ' .', ' . ', ' \"', '\".']\n",
      "0 Dígitos 920 ['22', '20', '01', '00', '21', '12', '16', '38', '020', '521']\n",
      "3 Símbolos 227 ['}}', '((', '))', '])', '>>', '}{', '})', ')}', ']}', '<{']\n",
      "5 Desconocidos 5744 ['ɾ', 'ˈ', 'ː', 'ž', 'č', '\\u200b', '«', '»', '“', '”']\n",
      "4 Especiales 8 ['\\t', '\\t\\t', '\\t\\t\\t', '\\x08', '\\x07', '\\x0c', '\\r', '\\x0b']\n"
     ]
    }
   ],
   "source": [
    "tokens_path = r\"D:\\datos\\tokens_ttkn_id_v20241208.pkl\"\n",
    "print(f\"Leyendo {tokens_path}\")\n",
    "with open(tokens_path, \"rb\") as file:\n",
    "    tokens_dict = pickle.load(file)\n",
    "print(\"Nada más leer:\")\n",
    "for ttkn in tokens_dict:\n",
    "    print(ttkn, K.TTKN_DESC[ttkn], len(tokens_dict[ttkn]), [x for x in tokens_dict[ttkn]][:10])\n",
    "tokens_dict[K.TTKN_SIS] = {token:K.MAIN_TOKEN_DICT[token] for token in K.MAIN_TOKEN_DICT}\n",
    "print(\"Arreglado:\")\n",
    "for ttkn in tokens_dict:\n",
    "    print(ttkn, K.TTKN_DESC[ttkn], len(tokens_dict[ttkn]), [x for x in tokens_dict[ttkn]][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vamos por ttkn=6 (Del Sistema)\n",
      "Vamos por ttkn=1 (Letras)\n",
      "Vamos por ttkn=2 (Separadores)\n",
      "Vamos por ttkn=0 (Dígitos)\n",
      "Vamos por ttkn=3 (Símbolos)\n",
      "Vamos por ttkn=5 (Desconocidos)\n",
      "Vamos por ttkn=4 (Especiales)\n",
      "El máximo token_id es 95743 log: 5 -> 10^x = 100000\n",
      "Generados 21405 tokens_id a reales\n",
      "2 2e-05 2 {'token': '<|LOWER|>', 'ttkn': 6}\n",
      "51 0.00051 51 {'token': '\\t\\t', 'ttkn': 4}\n",
      "1230 0.0123 1230 {'token': '\",\"', 'ttkn': 2}\n",
      "5132 0.05132 5132 {'token': '$)', 'ttkn': 3}\n",
      "41235 0.41235 41235 {'token': 'ner', 'ttkn': 1}\n",
      "93256 0.93256 93256 {'token': 'ṯ', 'ttkn': 5}\n"
     ]
    }
   ],
   "source": [
    "# Recuperamos la informacion de tipo de token (ttkn) junto al token\n",
    "ids_token = {}\n",
    "MAX_TOKEN_ID = 0\n",
    "for ttkn in tokens_dict:\n",
    "    print(f\"Vamos por ttkn={ttkn} ({K.TTKN_DESC[ttkn]})\")\n",
    "    for token in tokens_dict[ttkn]:\n",
    "        token_id = tokens_dict[ttkn][token]\n",
    "        ids_token[token_id] = {\"token\": token, \"ttkn\": ttkn}\n",
    "        if token_id > MAX_TOKEN_ID:\n",
    "            MAX_TOKEN_ID = tokens_dict[ttkn][token]\n",
    "print(f\"El máximo token_id es {MAX_TOKEN_ID} log: {round(log(MAX_TOKEN_ID, 10))} -> 10^x = {10**round(log(MAX_TOKEN_ID, 10))}\")\n",
    "TOKEN_DIVISOR = 10**round(log(MAX_TOKEN_ID, 10))\n",
    "# Generamos los valores reales\n",
    "token_id_real = {}\n",
    "for token_id in ids_token:\n",
    "    token_id_real[token_id] = round(token_id / TOKEN_DIVISOR, 7)\n",
    "print(f\"Generados {len(token_id_real)} tokens_id a reales\")\n",
    "def token_real_id(token_real):\n",
    "    token_id = int(round(token_real, 7) * TOKEN_DIVISOR)\n",
    "    if token_id in token_id_real:\n",
    "        return token_id\n",
    "    else:\n",
    "        if token_id + 1 in token_id_real and token_id_real[token_id+1] == round(token_real,7):\n",
    "            return token_id + 1\n",
    "        else:\n",
    "            return token_id - 1\n",
    "#Samples\n",
    "for token_id in [2, 51, 1230, 5132, 41235, 93256]:\n",
    "    while token_id not in token_id_real:\n",
    "        token_id += 1\n",
    "        if token_id > MAX_TOKEN_ID:\n",
    "            break\n",
    "    treal = token_id_real[token_id]\n",
    "    print(token_id, treal, token_real_id(treal), ids_token[token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40999, 40014]\n",
      "['ho', 'la']\n",
      "Hola\n",
      "HOLA\n"
     ]
    }
   ],
   "source": [
    "def tokeniza(n_grama, ttkn):\n",
    "    global tokens_dict\n",
    "    tokenizado = []\n",
    "    for token in tokens_dict[ttkn]:\n",
    "        if token in n_grama:\n",
    "            for subn in n_grama.split(token):\n",
    "                if subn and subn > \"\":\n",
    "                    tokenizado.extend(tokeniza(subn, ttkn))\n",
    "                tokenizado.append(tokens_dict[ttkn][token])\n",
    "            tokenizado.pop(-1)\n",
    "            break\n",
    "    return tokenizado\n",
    "# Test\n",
    "print(tokeniza(\"hola\", 1))\n",
    "print([ids_token[token_id][\"token\"] for token_id in tokeniza(\"hola\", 1)])\n",
    "\n",
    "def detokeniza(tokenizado):\n",
    "    global ids_token\n",
    "    detokenizado = \"\"\n",
    "    token_sis = \"\"\n",
    "    for token_ir in tokenizado:\n",
    "        \n",
    "        token = \"\"\n",
    "        if token_ir < 1.0:\n",
    "            token_id = token_real_id(token_ir)\n",
    "        else:\n",
    "            token_id = token_ir\n",
    "        if ids_token[token_id][\"ttkn\"] == K.TTKN_SIS:\n",
    "            token_sis = ids_token[token_id][\"token\"]\n",
    "        else:\n",
    "            token = ids_token[token_id][\"token\"]\n",
    "            if token_sis == K.UPPER_TOKEN:\n",
    "                token = \"\" + token.upper()\n",
    "            elif token_sis == K.CAP_TOKEN:\n",
    "                token = token[0].upper() + token[1:]\n",
    "            token_sis = \"\"\n",
    "        detokenizado += token\n",
    "    return detokenizado\n",
    "# Test\n",
    "print(detokeniza([3, 40999, 40014]))\n",
    "print(detokeniza([1, 40999, 1, 40014]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 40061, 54104, 1329, 40109, 1329, 40018, 54103, 1004, 40152, 54111, 1329, 54104, 40142, 1329, 5078, 40180, 5079, 1338, 3, 54119, 40070, 54112, 1329, 40061, 54103, 1329, 40109, 1329, 40018, 54103, 1329, 40036, 1329, 5083, 1, 5076, 1329, 20059, 20187, 20916, 1335, 20072]\n",
      "[3e-05, 0.40061, 0.54104, 0.01329, 0.40109, 0.01329, 0.40018, 0.54103, 0.01004, 0.40152, 0.54111, 0.01329, 0.54104, 0.40142, 0.01329, 0.05078, 0.4018, 0.05079, 0.01338, 3e-05, 0.54119, 0.4007, 0.54112, 0.01329, 0.40061, 0.54103, 0.01329, 0.40109, 0.01329, 0.40018, 0.54103, 0.01329, 0.40036, 0.01329, 0.05083, 1e-05, 0.05076, 0.01329, 0.20059, 0.20187, 0.20916, 0.01335, 0.20072]\n",
      "['<|CAP|>', 'est', 'o', ' ', 'es', ' ', 'un', 'a', ', ', 'ha', 'y', ' ', 'o', 'tra', ' ', '¿', 'no', '?', '\\n', '<|CAP|>', 'p', 'ue', 's', ' ', 'est', 'a', ' ', 'es', ' ', 'un', 'a', ' ', 'más', ' ', '¡', '<|UPPER|>', '!', ' ', '23', '58', '7', ',', '33']\n",
      "Esto es una, hay otra ¿no?\n",
      "Pues esta es una más ¡! 23587,33\n",
      "118\n",
      "Memoria: svmem(total=12649299968, available=1381879808, percent=89.1, used=11267420160, free=1381879808)\n"
     ]
    }
   ],
   "source": [
    "tokenizado = []\n",
    "texto = \"Esto es una, hay otra ¿no?\\nPues esta es una más ¡SEGURO! 23587,33\"\n",
    "for ttkn, n_grama in splitter.split_by_type(texto, 7):\n",
    "    valores = splitter.normalize_n_gram(n_grama)\n",
    "    if len(valores) > 1:\n",
    "        tokenizado.append(K.MAIN_TOKEN_DICT[valores[0]])\n",
    "    tokenizado.extend(tokeniza(valores[-1], ttkn))\n",
    "print(tokenizado)\n",
    "print([token_id_real[x] for x in tokenizado])\n",
    "print([ids_token[token_id][\"token\"] for token_id in tokenizado])\n",
    "print(detokeniza(tokenizado))\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo desde: D:\\datos\\wiki_1909627_entradas.pkl\n",
      "0\n",
      "Memoria: svmem(total=12649299968, available=1524199424, percent=88.0, used=11125100544, free=1524199424)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "wiki_path = r\"D:\\datos\\wiki_1909627_entradas.pkl\"\n",
    "print(f\"Leyendo desde: {wiki_path}\")\n",
    "with open(wiki_path, \"rb\") as wiki_file:\n",
    "    wiki_data = pickle.load(wiki_file)\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos en wiki: 1909627\n"
     ]
    }
   ],
   "source": [
    "print(f\"Documentos en wiki: {len(wiki_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2>Tokenizando Texto</h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_tokenized = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltar  = 0\n",
    "recoge = 10000 # Entradas de la wikipedia con sus textos\n",
    "limite = saltar + recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Realizamos varias pasadas para ir acumulando n-gramas</h3>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llevamos 10000 entradas de 10000 (100.0%)\n",
      "Con 16383634 tokens generados\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recogemos {recoge} entradas a partir de {saltar}\")\n",
    "cont_entradas = 0\n",
    "cont_tokens = 0\n",
    "max_len_allowed=7\n",
    "for entrada in wiki_data:\n",
    "    if cont_entradas > saltar:\n",
    "        texto_tokenized[cont_entradas] = []\n",
    "        tokenizado = []\n",
    "        for ttkn, n_grama in splitter.split_by_type(entrada, max_len_allowed):\n",
    "            valores = splitter.normalize_n_gram(n_grama)\n",
    "            if len(valores) > 1:\n",
    "                tokenizado.append(K.MAIN_TOKEN_DICT[valores[0]])\n",
    "            tokenizado.extend(tokeniza(valores[-1], ttkn))\n",
    "        texto_tokenized[cont_entradas].append([token_id_real[x] for x in tokenizado])\n",
    "        cont_tokens += len(tokenizado)\n",
    "        for linea in wiki_data[entrada]:\n",
    "            tokenizado = []\n",
    "            for ttkn, n_grama in splitter.split_by_type(linea.decode(\"utf-8\"), max_len_allowed):\n",
    "                valores = splitter.normalize_n_gram(n_grama)\n",
    "                if len(valores) > 1:\n",
    "                    tokenizado.append(K.MAIN_TOKEN_DICT[valores[0]])\n",
    "                tokenizado.extend(tokeniza(valores[-1], ttkn))\n",
    "            texto_tokenized[cont_entradas].append([token_id_real[x] for x in tokenizado])\n",
    "            cont_tokens += len(tokenizado)\n",
    "        if cont_entradas > limite:\n",
    "            break\n",
    "        if cont_entradas % 1000 == 0:\n",
    "            clear_output(True)\n",
    "            print(f\"Llevamos {cont_entradas} entradas de {limite} ({round(100*cont_entradas/limite, 2)}%)\")\n",
    "            print(f\"Con {cont_tokens} tokens generados\")\n",
    "    cont_entradas += 1\n",
    "salta = limite\n",
    "limite += recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px dashed #aaa;\"/>\n",
    "<center><h2>Liberamos memoria</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria: svmem(total=12649299968, available=3535872000, percent=72.0, used=9113427968, free=3535872000)\n",
      "187\n",
      "Memoria: svmem(total=12649299968, available=3725950976, percent=70.5, used=8923348992, free=3725950976)\n"
     ]
    }
   ],
   "source": [
    "print(\"Memoria:\", psutil.virtual_memory())\n",
    "del wiki_data\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Memoria: svmem(total=12649299968, available=7463751680, percent=41.0, used=5185548288, free=7463751680)\n"
     ]
    }
   ],
   "source": [
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando D:\\datos\\texto_tokenizado_v20241208.pkl\n"
     ]
    }
   ],
   "source": [
    "tokens_path = r\"D:\\datos\\texto_tokenizado_v20241208.pkl\"\n",
    "print(f\"Guardando {tokens_path}\")\n",
    "with open(tokens_path, \"wb\") as file:\n",
    "    pickle.dump(texto_tokenized, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color: red; height: 2; border: 3px dotted;\"/>\n",
    "<center><h1 style=\"color: yellow;\">Comienza la fiesta</h1></center>\n",
    "<hr style=\"color: red; width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El primer caracter DIGITO está en el token-r = 0.2\n",
      "El primer caracter LETRA está en el token-r = 0.4\n",
      "El primer caracter DESCONOCIDO está en el token-r = 0.9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FIRST_DIG_TOKEN_R = 0\n",
    "for token in tokens_dict[K.TTKN_DIG]:\n",
    "    FIRST_DIG_TOKEN_R = token_id_real[tokens_dict[K.TTKN_DIG][token]]\n",
    "    break\n",
    "print(f\"El primer caracter DIGITO está en el token-r = {FIRST_DIG_TOKEN_R}\")\n",
    "FIRST_LET_TOKEN_R = 0\n",
    "for token in tokens_dict[K.TTKN_LET]:\n",
    "    FIRST_LET_TOKEN_R = token_id_real[tokens_dict[K.TTKN_LET][token]]\n",
    "    break\n",
    "print(f\"El primer caracter LETRA está en el token-r = {FIRST_LET_TOKEN_R}\")\n",
    "FIRST_UNK_TOKEN_R = 0\n",
    "for token in tokens_dict[K.TTKN_UNK]:\n",
    "    FIRST_UNK_TOKEN_R = token_id_real[tokens_dict[K.TTKN_UNK][token]]\n",
    "    break\n",
    "print(f\"El primer caracter DESCONOCIDO está en el token-r = {FIRST_UNK_TOKEN_R}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de texto para entrnamiento predictivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m gnt \u001b[38;5;241m=\u001b[39m GetNextToken(texto_tokenized)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgnt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m, in \u001b[0;36mGetNextToken.take_next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m token_id_real, K\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mart_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_l \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_c \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m         token_id_real[K\u001b[38;5;241m.\u001b[39mMAIN_TOKEN_DICT[K\u001b[38;5;241m.\u001b[39mSTART_TOKEN]],\n\u001b[1;32m---> 15\u001b[0m         \u001b[43mtokenizado\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mart_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mi_l\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_c]\n\u001b[0;32m     16\u001b[0m     ]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m [tokenizado[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mart_id][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_l][\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mi_c]]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "class GetNextToken():\n",
    "    def __init__(self, tokenizado):\n",
    "        self.art_id = 0\n",
    "        self.i_l = 0\n",
    "        self.i_c = 0\n",
    "    def reset(self):\n",
    "        self.art_id = 0\n",
    "        self.i_l = 0\n",
    "        self.i_c = 0\n",
    "    def take_next(self):\n",
    "        global token_id_real, K\n",
    "        if self.art_id == 0 and self.i_l == 0 and self.i_c == 0:\n",
    "            next_token = [\n",
    "                token_id_real[K.MAIN_TOKEN_DICT[K.START_TOKEN]],\n",
    "                tokenizado[self.art_id][self.i_l][self.i_c]\n",
    "            ]\n",
    "        else:\n",
    "            next_token = [tokenizado[self.art_id][self.i_l][self.i_c]]\n",
    "        self.i_c += 1\n",
    "        if self.i_c >= len(tokenizado[self.art_id][self.i_l]):\n",
    "            self.i_c = 0\n",
    "            self.i_l += 1\n",
    "            if self.i_l >= len(tokenizado[self.art_id]):\n",
    "                self.i_l = 0\n",
    "                self.art_id += 1\n",
    "                if self.art_id >= len(tokenizado):\n",
    "                    self.art_id = 0\n",
    "                    next_token.append(token_id_real[K.MAIN_TOKEN_DICT[K.EOF_TOKEN]])\n",
    "                else:\n",
    "                    next_token.append(token_id_real[K.MAIN_TOKEN_DICT[K.CONTINUE_TOKEN]])\n",
    "            else:\n",
    "                next_token.append(token_id_real[K.MAIN_TOKEN_DICT[K.CONTINUE_TOKEN]])\n",
    "        return next_token\n",
    "# TEst\n",
    "gnt = GetNextToken(texto_tokenized)\n",
    "for i in range(10):\n",
    "    print(gnt.take_next())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for art_id in texto_tokenized:\n",
    "    xline = [token_id_real[K.MAIN_TOKEN_DICT[K.START_TOKEN]]]\n",
    "    i_l = 0\n",
    "    while texto_tokenized[art_id][i_t] < FIRST_LET_TOKEN_R:\n",
    "        xline.append(texto_tokenized[art_id][i_t])\n",
    "        i_t += 1\n",
    "    for token_r in texto_tokenized[art_id]:\n",
    "        if token_r >= FIRST_LET_TOKEN_R:\n",
    "\n",
    "    #xline.append(token_id_real[K.MAIN_TOKEN_DICT[K.CONTINUE_TOKEN]])\n",
    "    #xline.append(token_id_real[K.MAIN_TOKEN_DICT[K.EOF_TOKEN]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
