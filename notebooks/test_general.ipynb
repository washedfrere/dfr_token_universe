{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import psutil\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dfrtokenuniverse.splitter\n",
    "import dfrtokenuniverse.word_inventory\n",
    "import dfrtokenuniverse.constantes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recarga librerías propias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dfrtokenuniverse.splitter)\n",
    "importlib.reload(dfrtokenuniverse.word_inventory)\n",
    "importlib.reload(dfrtokenuniverse.constantes)\n",
    "from dfrtokenuniverse.splitter import TextSplitter\n",
    "from dfrtokenuniverse.word_inventory import WordInventory\n",
    "from dfrtokenuniverse.constantes import KDfrNlp\n",
    "K = KDfrNlp()\n",
    "splitter = TextSplitter()\n",
    "invent = WordInventory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo splitter e inventario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dic = {}\n",
    "for ttkn, n_grama in splitter.split_by_type(\"Esto es una, hay otra ¿no?\\nPues esta es una más ¡SEGURO! 23587,33\", 4):\n",
    "    invent.normalize_and_count(ttkn, n_grama, a_dic)\n",
    "print(a_dic)\n",
    "for ttkn in a_dic:\n",
    "    for n_grama in a_dic[ttkn]:\n",
    "        p_ngrama = n_grama.replace(\"\\n\", \"¬\")\n",
    "        print(f\"{K.TTKN_DESC[ttkn]} contiene '{p_ngrama}' con {a_dic[ttkn][n_grama]} apariciones\")\n",
    "del a_dic\n",
    "del ttkn\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "wiki_path = r\"D:\\datos\\wiki_1909627_entradas.pkl\"\n",
    "print(f\"Leyendo desde: {wiki_path}\")\n",
    "with open(wiki_path, \"rb\") as wiki_file:\n",
    "    wiki_data = pickle.load(wiki_file)\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Documentos en wiki: {len(wiki_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2>Diccionario de n-gramas</h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ngramas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltar  = 0\n",
    "recoge = 100000 # Entradas de la wikipedia con sus textos\n",
    "limite = saltar + recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Realizamos varias pasadas para ir acumulando n-gramas</h3>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Recogemos {recoge} entradas a partir de {saltar}\")\n",
    "cont_entradas = 0\n",
    "max_len_allowed=35\n",
    "for entrada in wiki_data:\n",
    "    if cont_entradas > saltar:\n",
    "        for ttkn, n_grama in splitter.split_by_type(entrada, max_len=max_len_allowed):\n",
    "            invent.normalize_and_count(ttkn, n_grama, dic_ngramas)\n",
    "        for linea in wiki_data[entrada]:\n",
    "            for ttkn, n_grama in splitter.split_by_type(linea.decode(\"utf-8\"), max_len=max_len_allowed):\n",
    "                invent.normalize_and_count(ttkn, n_grama, dic_ngramas)\n",
    "        if cont_entradas > limite:\n",
    "            break\n",
    "        if cont_entradas % 1000 == 0:\n",
    "            clear_output(True)\n",
    "            print(f\"Llevamos {cont_entradas} entradas de {limite} ({round(100*cont_entradas/limite, 2)}%)\")\n",
    "            for ttkn in dic_ngramas:\n",
    "                print(f\"- {len(dic_ngramas[ttkn])} n-gramas recolectados del tipo {K.TTKN_DESC[ttkn]}\")\n",
    "    cont_entradas += 1\n",
    "salta = limite\n",
    "limite += recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px dashed #aaa;\"/>\n",
    "<center><h2>Liberamos memoria</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Memoria:\", psutil.virtual_memory())\n",
    "del wiki_data\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color: red; height: 2; border: 3px dotted;\"/>\n",
    "<center><h1 style=\"color: yellow;\">Comienza la fiesta</h1></center>\n",
    "<hr style=\"color: red; width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ngrama_len = 0\n",
    "max_ngrama_ngrama = \"\"\n",
    "max_ngrama_ttkn = \"\"\n",
    "maximos = []\n",
    "for ttkn in dic_ngramas:\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        if len(n_grama) > max_ngrama_len:\n",
    "            max_ngrama_len = len(n_grama)\n",
    "            max_ngrama_ngrama = n_grama\n",
    "            max_ngrama_ttkn = ttkn\n",
    "    maximos = [n_grama.replace(\"\\n\", \"¬\") for n_grama in sorted(dic_ngramas[ttkn], key=lambda x:dic_ngramas[ttkn][x], reverse=True)[:15]]\n",
    "    print(f\"Para el tipo {K.TTKN_DESC[ttkn].rjust(12)} tenemos {str(len(dic_ngramas[ttkn])).rjust(11)}: {maximos}\")\n",
    "print(f\"El ngrama más largo, con {max_ngrama_len} es del tipo {K.TTKN_DESC[ttkn]} ({ttkn}): <|{max_ngrama_ngrama.encode()}|>\")\n",
    "del maximos\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contabilizacion de monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_monogramas = {ttkn:{} for ttkn in dic_ngramas}\n",
    "acum_monogramas = {ttkn:0 for ttkn in dic_ngramas}\n",
    "for ttkn in dic_ngramas:\n",
    "    print(f\"Contando tipo {K.TTKN_DESC[ttkn].rjust(12)}...\", end=\"\")\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        for monograma in n_grama:\n",
    "            invent._dict_counter(ttkn, monograma, prob_monogramas)\n",
    "            acum_monogramas[ttkn] += 1\n",
    "    print(f\" {str(acum_monogramas[ttkn]).rjust(11)} apariciones (probabilidad media: {round(100 * len(prob_monogramas[ttkn]) / acum_monogramas[ttkn], 5)}%)\")\n",
    "print(\"Probabilidades...\")\n",
    "prob_monogramas = {ttkn:{monograma: prob_monogramas[ttkn][monograma] / acum_monogramas[ttkn] for monograma in prob_monogramas[ttkn]} for ttkn in prob_monogramas}\n",
    "print(\"¡Terminado!\")\n",
    "del acum_monogramas\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probabilidad de monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ttkn in prob_monogramas:\n",
    "    maximos = [monograma.replace(\"\\n\", \" \") for monograma in sorted(prob_monogramas[ttkn], key=lambda x: prob_monogramas[ttkn][x], reverse=True)[:10]]\n",
    "    print(f\"Para {K.TTKN_DESC[ttkn].rjust(15)}: {', '.join([f'<{x}>: {round(prob_monogramas[ttkn][x], 7)}' for x in maximos])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinatorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragmentos = {ttkn: {} for ttkn in dic_ngramas}\n",
    "fragmentos_n_grama = {ttkn: {} for ttkn in dic_ngramas}\n",
    "max_len_frag = 5\n",
    "print(f\"Fragmentando ...\".rjust(50) + \"\".join([\"|\"]*100))\n",
    "for ttkn in dic_ngramas:\n",
    "    print(f\"Fragmentando {len(dic_ngramas[ttkn])} n-gramas de {K.TTKN_DESC[ttkn]}\".rjust(50), end=\"\")\n",
    "    cada = 1+ len(dic_ngramas[ttkn]) // 100\n",
    "    cont = 0\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        for l_n in range(2, min([len(n_grama), max_len_frag]) - 1):\n",
    "            for i_ini in range(len(n_grama) - l_n):\n",
    "                if n_grama[i_ini:i_ini+l_n] in fragmentos[ttkn]:\n",
    "                    fragmentos[ttkn][n_grama[i_ini:i_ini+l_n]] += 1\n",
    "                    if n_grama in fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]]:\n",
    "                        fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]][n_grama] += 1\n",
    "                    else:\n",
    "                        fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]][n_grama] = 1\n",
    "                else:\n",
    "                    fragmentos[ttkn][n_grama[i_ini:i_ini+l_n]] = 1\n",
    "                    fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]] = {n_grama: 1}\n",
    "        cont += 1\n",
    "        if cont % cada == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    print(\"!\")\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragmentos óptimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acum_frag_n_grama = {ttkn: sum([len(fragmentos_n_grama[ttkn][x]) for x in fragmentos_n_grama[ttkn]]) for ttkn in fragmentos_n_grama}\n",
    "acum_fragmentos = {ttkn: sum([fragmentos[ttkn][x] for x in fragmentos[ttkn]]) for ttkn in fragmentos}\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usados = {}\n",
    "excedentes = {}\n",
    "n_grama = \"Esto es una prueba\"\n",
    "evaluados = [\"Es\", \"to\", \"ta\", \"es\", \"una\", \"pru\", \"eba\", \".\", \" \"]\n",
    "n_grama = \"Esto es una prueba\"\n",
    "invent.explota_fragmento(n_grama, usados, excedentes, evaluados)\n",
    "n_grama = \"Esto es otra prueba\"\n",
    "invent.explota_fragmento(n_grama, usados, excedentes, evaluados)\n",
    "print(\"usados\", usados)\n",
    "print(\"excedentes\", excedentes)\n",
    "del usados\n",
    "del evaluados\n",
    "del excedentes\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluador de fragmentos en n-gramas para obtener tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragmentos_usados_ttkn = {ttkn: {} for ttkn in fragmentos}\n",
    "excedentes_ttkn = {ttkn: {} for ttkn in fragmentos}\n",
    "fragmentos_evaluados_ttkn = {}\n",
    "print(f\"Evaluando fragmentos sobre tokens ...\".rjust(70) + \"\".join([\"|\"]*100))\n",
    "for ttkn in dic_ngramas:\n",
    "    print(f\"Ordenando {len(fragmentos[ttkn])} fragmentos de {K.TTKN_DESC[ttkn]}...\", end=\"\")\n",
    "    fragmentos_usados = {}\n",
    "    excedentes = {}\n",
    "    previos = sorted(\n",
    "        fragmentos[ttkn],\n",
    "        key=lambda x: (\n",
    "            0.55 + len(x) * (1.0 + len(x) - len(set(x))) / max_len_frag\n",
    "        ) ** (\n",
    "            (\n",
    "                1.0 * (fragmentos[ttkn][x] / acum_fragmentos[ttkn]) +\n",
    "                3.0 * (len(fragmentos_n_grama[ttkn][x]) /acum_frag_n_grama[ttkn])\n",
    "            ) / 4.0\n",
    "        ),\n",
    "        reverse=True\n",
    "    )\n",
    "    print(f\"Fragmentos previos: {len(previos)} = {previos[:10]}\")\n",
    "    evaluados = []\n",
    "    cont = 0\n",
    "    cada = 1 + len(previos) // 100\n",
    "    print(f\"Optimizando {len(previos)} n-gramas de {K.TTKN_DESC[ttkn]} cada {cada} un punto.\".rjust(70), end=\"\")\n",
    "    no_esta = True\n",
    "    for fragmento in previos:\n",
    "        no_esta = True\n",
    "        for x in evaluados:\n",
    "            if len(x) < len(fragmento) and x in fragmento:\n",
    "                no_esta = False\n",
    "                break\n",
    "        if no_esta:\n",
    "            evaluados.append(fragmento)\n",
    "        cont += 1\n",
    "        if cont % cada == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    del previos\n",
    "    print(\"!\")\n",
    "    print(f\"Fragmentos evaluados: {len(evaluados)} = {evaluados[:10]}\")\n",
    "    cada = 1 + len(dic_ngramas[ttkn]) // 100\n",
    "    print(f\"Evaluando {len(dic_ngramas[ttkn])} n-gramas de {K.TTKN_DESC[ttkn]} cada {cada} un punto.\".rjust(70), end=\"\")\n",
    "    cont = 0\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        if isinstance(n_grama, str) and len(n_grama) < 50:\n",
    "            invent.explota_fragmento(n_grama, fragmentos_usados, excedentes, evaluados)\n",
    "        cont += 1\n",
    "        if cont % cada == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    print(\"!\")\n",
    "    print(\"Recogiendo datos...\")\n",
    "    fragmentos_usados_ttkn[ttkn] = fragmentos_usados.copy()\n",
    "    print(f\"Fragmentos usados en {K.TTKN_DESC[ttkn]}: {len(fragmentos_usados)}\")\n",
    "    excedentes_ttkn[ttkn] = excedentes.copy()\n",
    "    print(f\"Excedentes en {K.TTKN_DESC[ttkn]}: {len(excedentes)}\")\n",
    "    del fragmentos_usados\n",
    "    del excedentes\n",
    "    print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado (si procede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fragmentos_usados_path = r\"D:\\datos\\tokens_usados.pkl\"\n",
    "excedentes_path = r\"D:\\datos\\tokens_excedentes.pkl\"\n",
    "print(f\"Guardando {fragmentos_usados_path}\")\n",
    "with open(fragmentos_usados_path, \"wb\") as file:\n",
    "    pickle.dump(fragmentos_usados_ttkn, file)\n",
    "print(f\"Guardando {excedentes_path}\")\n",
    "with open(excedentes_path, \"wb\") as file:\n",
    "    pickle.dump(excedentes_ttkn, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diccionario de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orden y numeración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttkn_start_id = {\n",
    "    K.TTKN_ESP: 50,\n",
    "    K.TTKN_SEP: 1000,\n",
    "    K.TTKN_SIM: 5000,\n",
    "    K.TTKN_DIG: 20000,\n",
    "    K.TTKN_LET: 40000,\n",
    "    K.TTKN_UNK: 90000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empezamos por los tokens del sistema\n",
    "tokens = {K.TTKN_SIS:token_id for token_id in K.MAIN_TOKEN_DICT}\n",
    "# Inicializamos los tipos de token previendo su rellenado posterior\n",
    "for ttkn in fragmentos:\n",
    "    tokens[ttkn] = {}\n",
    "# Iniciamos desde el token_id = 10 como minimo\n",
    "token_id = 10\n",
    "# A rellenar...\n",
    "for ttkn in ttkn_start_id:\n",
    "    if ttkn not in fragmentos_usados_ttkn:\n",
    "        continue\n",
    "    # Situamos la numeracion optimizada\n",
    "    if token_id < ttkn_start_id[ttkn]:\n",
    "        token_id = ttkn_start_id[ttkn]\n",
    "    else:\n",
    "        token_id += ttkn_start_id[ttkn]\n",
    "    print(f\"Para {K.TTKN_DESC[ttkn]} empezamos por {token_id}\", end=\",\")\n",
    "    # Incluimos en esa trancha de ids los tokens\n",
    "    for fragmento in fragmentos_usados_ttkn[ttkn]:\n",
    "        tokens[ttkn][fragmento] = token_id\n",
    "        token_id += 1\n",
    "    print(f\" usados ({token_id-1}),\", end=\"\")\n",
    "    for excedente in excedentes_ttkn[ttkn]:\n",
    "        tokens[ttkn][excedente] = token_id\n",
    "        token_id += 1\n",
    "    print(f\" excedentes ({token_id-1}),\", end=\"\")\n",
    "    # Que no se nos olviden los monogramas encontrados\n",
    "    if ttkn in prob_monogramas:\n",
    "        for monograma in sorted(prob_monogramas[ttkn], key=lambda x: prob_monogramas[ttkn][x], reverse=True):\n",
    "            if monograma not in tokens[ttkn]:\n",
    "                tokens[ttkn][monograma] = token_id\n",
    "                token_id += 1\n",
    "    print(f\" monogramas ({token_id-1}),\", end=\"\")\n",
    "    # Para que quede todo relleno, los de las constantes\n",
    "    if ttkn in K.TIPOS_TKN:\n",
    "        for caracter in K.TIPOS_TKN[ttkn]:\n",
    "            if caracter.lower() not in tokens[ttkn]:\n",
    "                tokens[ttkn][caracter.lower()] = token_id\n",
    "                token_id += 1\n",
    "    print(f\" y constantes. Terminamos por {token_id-1}. Tenemos {len(tokens[ttkn])}\")\n",
    "    print(f\"Llevamos {sum([len(tokens[x]) for x in tokens])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardado de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_path = r\"D:\\datos\\tokens_ttkn_id_v20241208.pkl\"\n",
    "print(f\"Guardando {tokens_path}\")\n",
    "with open(tokens_path, \"wb\") as file:\n",
    "    pickle.dump(tokens, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
