{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:grey;\"> Generador de Tokens </h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Memoria: svmem(total=12649299968, available=7958265856, percent=37.1, used=4691034112, free=7958265856)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import gc\n",
    "import psutil\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dfrtokenuniverse.splitter\n",
    "import dfrtokenuniverse.word_inventory\n",
    "import dfrtokenuniverse.constantes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recarga librerías propias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(dfrtokenuniverse.splitter)\n",
    "importlib.reload(dfrtokenuniverse.word_inventory)\n",
    "importlib.reload(dfrtokenuniverse.constantes)\n",
    "from dfrtokenuniverse.splitter import TextSplitter\n",
    "from dfrtokenuniverse.word_inventory import WordInventory\n",
    "from dfrtokenuniverse.constantes import KDfrNlp\n",
    "K = KDfrNlp()\n",
    "splitter = TextSplitter()\n",
    "invent = WordInventory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo splitter e inventario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'esto': 1, 'es': 2, 'una': 2, 'hay': 1, 'otra': 1, 'no': 1, 'pues': 1, 'esta': 1, 'más': 1, 'segu': 1, 'ro': 1}, 2: {' ': 10, ', ': 1, '\\n': 1, ',': 1}, 3: {'¿': 1, '?': 1, '¡': 1, '!': 1}, 0: {'2358': 1, '7': 1, '33': 1}}\n",
      "Letras contiene 'esto' con 1 apariciones\n",
      "Letras contiene 'es' con 2 apariciones\n",
      "Letras contiene 'una' con 2 apariciones\n",
      "Letras contiene 'hay' con 1 apariciones\n",
      "Letras contiene 'otra' con 1 apariciones\n",
      "Letras contiene 'no' con 1 apariciones\n",
      "Letras contiene 'pues' con 1 apariciones\n",
      "Letras contiene 'esta' con 1 apariciones\n",
      "Letras contiene 'más' con 1 apariciones\n",
      "Letras contiene 'segu' con 1 apariciones\n",
      "Letras contiene 'ro' con 1 apariciones\n",
      "Separadores contiene ' ' con 10 apariciones\n",
      "Separadores contiene ', ' con 1 apariciones\n",
      "Separadores contiene '¬' con 1 apariciones\n",
      "Separadores contiene ',' con 1 apariciones\n",
      "Símbolos contiene '¿' con 1 apariciones\n",
      "Símbolos contiene '?' con 1 apariciones\n",
      "Símbolos contiene '¡' con 1 apariciones\n",
      "Símbolos contiene '!' con 1 apariciones\n",
      "Dígitos contiene '2358' con 1 apariciones\n",
      "Dígitos contiene '7' con 1 apariciones\n",
      "Dígitos contiene '33' con 1 apariciones\n",
      "23\n",
      "Memoria: svmem(total=12649299968, available=7957745664, percent=37.1, used=4691554304, free=7957745664)\n"
     ]
    }
   ],
   "source": [
    "a_dic = {}\n",
    "for ttkn, n_grama in splitter.split_by_type(\"Esto es una, hay otra ¿no?\\nPues esta es una más ¡SEGURO! 23587,33\", 4):\n",
    "    invent.normalize_and_count(ttkn, n_grama, a_dic)\n",
    "print(a_dic)\n",
    "for ttkn in a_dic:\n",
    "    for n_grama in a_dic[ttkn]:\n",
    "        p_ngrama = n_grama.replace(\"\\n\", \"¬\")\n",
    "        print(f\"{K.TTKN_DESC[ttkn]} contiene '{p_ngrama}' con {a_dic[ttkn][n_grama]} apariciones\")\n",
    "del a_dic\n",
    "del ttkn\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recuperamos texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo desde: D:\\datos\\wiki_1909627_entradas.pkl\n",
      "0\n",
      "Memoria: svmem(total=12649299968, available=1996402688, percent=84.2, used=10652897280, free=1996402688)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "wiki_path = r\"D:\\datos\\wiki_1909627_entradas.pkl\"\n",
    "print(f\"Leyendo desde: {wiki_path}\")\n",
    "with open(wiki_path, \"rb\") as wiki_file:\n",
    "    wiki_data = pickle.load(wiki_file)\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos en wiki: 1909627\n"
     ]
    }
   ],
   "source": [
    "print(f\"Documentos en wiki: {len(wiki_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h2>Diccionario de n-gramas</h2> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ngramas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltar  = 0\n",
    "recoge = 100000 # Entradas de la wikipedia con sus textos\n",
    "limite = saltar + recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Realizamos varias pasadas para ir acumulando n-gramas</h3>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llevamos 100000 entradas de 100000 (100.0%)\n",
      "- 573339 n-gramas recolectados del tipo Letras\n",
      "- 2469 n-gramas recolectados del tipo Separadores\n",
      "- 10350 n-gramas recolectados del tipo Dígitos\n",
      "- 410 n-gramas recolectados del tipo Símbolos\n",
      "- 5744 n-gramas recolectados del tipo Desconocidos\n",
      "- 3 n-gramas recolectados del tipo Especiales\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recogemos {recoge} entradas a partir de {saltar}\")\n",
    "cont_entradas = 0\n",
    "max_len_allowed=35\n",
    "for entrada in wiki_data:\n",
    "    if cont_entradas > saltar:\n",
    "        for ttkn, n_grama in splitter.split_by_type(entrada, max_len=max_len_allowed):\n",
    "            invent.normalize_and_count(ttkn, n_grama, dic_ngramas)\n",
    "        for linea in wiki_data[entrada]:\n",
    "            for ttkn, n_grama in splitter.split_by_type(linea.decode(\"utf-8\"), max_len=max_len_allowed):\n",
    "                invent.normalize_and_count(ttkn, n_grama, dic_ngramas)\n",
    "        if cont_entradas > limite:\n",
    "            break\n",
    "        if cont_entradas % 1000 == 0:\n",
    "            clear_output(True)\n",
    "            print(f\"Llevamos {cont_entradas} entradas de {limite} ({round(100*cont_entradas/limite, 2)}%)\")\n",
    "            for ttkn in dic_ngramas:\n",
    "                print(f\"- {len(dic_ngramas[ttkn])} n-gramas recolectados del tipo {K.TTKN_DESC[ttkn]}\")\n",
    "    cont_entradas += 1\n",
    "salta = limite\n",
    "limite += recoge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px dashed #aaa;\"/>\n",
    "<center><h2>Liberamos memoria</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria: svmem(total=12649299968, available=2215477248, percent=82.5, used=10433822720, free=2215477248)\n",
      "0\n",
      "Memoria: svmem(total=12649299968, available=8069984256, percent=36.2, used=4579315712, free=8069984256)\n"
     ]
    }
   ],
   "source": [
    "print(\"Memoria:\", psutil.virtual_memory())\n",
    "del wiki_data\n",
    "print(gc.collect())\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"color: red; height: 2; border: 3px dotted;\"/>\n",
    "<center><h1 style=\"color: yellow;\">Comienza la fiesta</h1></center>\n",
    "<hr style=\"color: red; width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para el tipo       Letras tenemos      573341: ['de', 'la', 'en', 'el', 'y', 'a', 'que', 'los', 'del', 'se', 'por', 'un', 'con', 'una', 'las']\n",
      "Para el tipo  Separadores tenemos        2469: [' ', ', ', '. ', ' , ', '.', ' . ', '  ', '-', '.  ', ' .', ' \"', ': ', ':', ',', '; ']\n",
      "Para el tipo      Dígitos tenemos       10350: ['1', '2', '3', '4', '5', '0', '8', '6', '10', '7', '9', '000', '20', '2010', '12']\n",
      "Para el tipo     Símbolos tenemos         410: ['(', ')', '%', '{', '[', ']', '}', 'º', '>', '&', '$', '#', 'ª', '?', '!']\n",
      "Para el tipo Desconocidos tenemos        5744: ['«', '»', '²', '”', '“', '—', '°', '–', '=', '′', '″', 'ö', 'ō', 'а', '’']\n",
      "Para el tipo   Especiales tenemos           3: ['\\t', '\\t\\t', '\\t\\t\\t']\n",
      "El ngrama más largo, con 35 es del tipo Especiales (4): <|b'cliominervareflexionestrayectoriaes'|>\n",
      "Limpiando caché de 0 objetos...\n",
      "Memoria: svmem(total=12649299968, available=8093806592, percent=36.0, used=4555493376, free=8093806592)\n"
     ]
    }
   ],
   "source": [
    "max_ngrama_len = 0\n",
    "max_ngrama_ngrama = \"\"\n",
    "max_ngrama_ttkn = \"\"\n",
    "maximos = []\n",
    "for ttkn in dic_ngramas:\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        if len(n_grama) > max_ngrama_len:\n",
    "            max_ngrama_len = len(n_grama)\n",
    "            max_ngrama_ngrama = n_grama\n",
    "            max_ngrama_ttkn = ttkn\n",
    "    maximos = [n_grama.replace(\"\\n\", \"¬\") for n_grama in sorted(dic_ngramas[ttkn], key=lambda x:dic_ngramas[ttkn][x], reverse=True)[:15]]\n",
    "    print(f\"Para el tipo {K.TTKN_DESC[ttkn].rjust(12)} tenemos {str(len(dic_ngramas[ttkn])).rjust(11)}: {maximos}\")\n",
    "print(f\"El ngrama más largo, con {max_ngrama_len} es del tipo {K.TTKN_DESC[ttkn]} ({ttkn}): <|{max_ngrama_ngrama.encode()}|>\")\n",
    "del maximos\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contabilizacion de monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contando tipo       Letras...     4727835 apariciones (probabilidad media: 0.00072%)\n",
      "Contando tipo  Separadores...       14553 apariciones (probabilidad media: 0.08933%)\n",
      "Contando tipo      Dígitos...       49053 apariciones (probabilidad media: 0.02039%)\n",
      "Contando tipo     Símbolos...        1220 apariciones (probabilidad media: 1.96721%)\n",
      "Contando tipo Desconocidos...        5744 apariciones (probabilidad media: 100.0%)\n",
      "Contando tipo   Especiales...           6 apariciones (probabilidad media: 16.66667%)\n",
      "Probabilidades...\n",
      "¡Terminado!\n",
      "Limpiando caché de 0 objetos...\n",
      "Memoria: svmem(total=12649299968, available=8090357760, percent=36.0, used=4558942208, free=8090357760)\n"
     ]
    }
   ],
   "source": [
    "prob_monogramas = {ttkn:{} for ttkn in dic_ngramas}\n",
    "acum_monogramas = {ttkn:0 for ttkn in dic_ngramas}\n",
    "for ttkn in dic_ngramas:\n",
    "    print(f\"Contando tipo {K.TTKN_DESC[ttkn].rjust(12)}...\", end=\"\")\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        for monograma in n_grama:\n",
    "            invent._dict_counter(ttkn, monograma, prob_monogramas)\n",
    "            acum_monogramas[ttkn] += 1\n",
    "    print(f\" {str(acum_monogramas[ttkn]).rjust(11)} apariciones (probabilidad media: {round(100 * len(prob_monogramas[ttkn]) / acum_monogramas[ttkn], 5)}%)\")\n",
    "print(\"Probabilidades...\")\n",
    "prob_monogramas = {ttkn:{monograma: prob_monogramas[ttkn][monograma] / acum_monogramas[ttkn] for monograma in prob_monogramas[ttkn]} for ttkn in prob_monogramas}\n",
    "print(\"¡Terminado!\")\n",
    "del acum_monogramas\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probabilidad de monogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para          Letras: <a>: 0.1240655, <e>: 0.0912153, <i>: 0.0836994, <o>: 0.0755189, <r>: 0.0742742, <n>: 0.0677342, <s>: 0.0664067, <t>: 0.052289, <l>: 0.0496308, <c>: 0.0432966\n",
      "Para     Separadores: < >: 0.5438741, <.>: 0.1528894, <\">: 0.0628736, <,>: 0.0507799, <'>: 0.0316773, <->: 0.027898, <:>: 0.0261115, <;>: 0.0199272, < >: 0.5438741, <_>: 0.0172473\n",
      "Para         Dígitos: <0>: 0.1473101, <1>: 0.1326932, <2>: 0.1122459, <3>: 0.0959778, <4>: 0.0929403, <5>: 0.0896581, <6>: 0.0869875, <7>: 0.0828084, <9>: 0.0803009, <8>: 0.0790777\n",
      "Para        Símbolos: <}>: 0.2696721, <)>: 0.1114754, <(>: 0.0827869, <{>: 0.0819672, <&>: 0.0778689, <]>: 0.0532787, <!>: 0.0491803, <?>: 0.0434426, <[>: 0.0344262, <|>: 0.0344262\n",
      "Para    Desconocidos: <ɾ>: 0.0001741, <ˈ>: 0.0001741, <ː>: 0.0001741, <ž>: 0.0001741, <č>: 0.0001741, <​>: 0.0001741, <«>: 0.0001741, <»>: 0.0001741, <“>: 0.0001741, <”>: 0.0001741\n",
      "Para      Especiales: <\t>: 1.0\n"
     ]
    }
   ],
   "source": [
    "for ttkn in prob_monogramas:\n",
    "    maximos = [monograma.replace(\"\\n\", \" \") for monograma in sorted(prob_monogramas[ttkn], key=lambda x: prob_monogramas[ttkn][x], reverse=True)[:10]]\n",
    "    print(f\"Para {K.TTKN_DESC[ttkn].rjust(15)}: {', '.join([f'<{x}>: {round(prob_monogramas[ttkn][x], 7)}' for x in maximos])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinatorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Fragmentando ...||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "            Fragmentando 573341 n-gramas de Letras...................................................................................................!\n",
      "         Fragmentando 2469 n-gramas de Separadores..................................................................................................!\n",
      "            Fragmentando 10350 n-gramas de Dígitos...................................................................................................!\n",
      "             Fragmentando 410 n-gramas de Símbolos..................................................................................!\n",
      "        Fragmentando 5744 n-gramas de Desconocidos...................................................................................................!\n",
      "             Fragmentando 3 n-gramas de Especiales...!\n",
      "Limpiando caché de 0 objetos...\n",
      "Memoria: svmem(total=12649299968, available=7895400448, percent=37.6, used=4753899520, free=7895400448)\n"
     ]
    }
   ],
   "source": [
    "fragmentos = {ttkn: {} for ttkn in dic_ngramas}\n",
    "fragmentos_n_grama = {ttkn: {} for ttkn in dic_ngramas}\n",
    "max_len_frag = 5\n",
    "print(f\"Fragmentando ...\".rjust(50) + \"\".join([\"|\"]*100))\n",
    "for ttkn in dic_ngramas:\n",
    "    print(f\"Fragmentando {len(dic_ngramas[ttkn])} n-gramas de {K.TTKN_DESC[ttkn]}\".rjust(50), end=\"\")\n",
    "    cada = 1+ len(dic_ngramas[ttkn]) // 100\n",
    "    cont = 0\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        for l_n in range(2, min([len(n_grama), max_len_frag]) - 1):\n",
    "            for i_ini in range(len(n_grama) - l_n):\n",
    "                if n_grama[i_ini:i_ini+l_n] in fragmentos[ttkn]:\n",
    "                    fragmentos[ttkn][n_grama[i_ini:i_ini+l_n]] += 1\n",
    "                    if n_grama in fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]]:\n",
    "                        fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]][n_grama] += 1\n",
    "                    else:\n",
    "                        fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]][n_grama] = 1\n",
    "                else:\n",
    "                    fragmentos[ttkn][n_grama[i_ini:i_ini+l_n]] = 1\n",
    "                    fragmentos_n_grama[ttkn][n_grama[i_ini:i_ini+l_n]] = {n_grama: 1}\n",
    "        cont += 1\n",
    "        if cont % cada == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    print(\"!\")\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fragmentos óptimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpiando caché de 0 objetos...\n",
      "Memoria: svmem(total=12649299968, available=7893495808, percent=37.6, used=4755804160, free=7893495808)\n"
     ]
    }
   ],
   "source": [
    "acum_frag_n_grama = {ttkn: sum([len(fragmentos_n_grama[ttkn][x]) for x in fragmentos_n_grama[ttkn]]) for ttkn in fragmentos_n_grama}\n",
    "acum_fragmentos = {ttkn: sum([fragmentos[ttkn][x] for x in fragmentos[ttkn]]) for ttkn in fragmentos}\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usados {'Es': 2, 'to': 2, 'es': 2, ' ': 5, 'una': 1, 'pru': 2, 'eba': 2}\n",
      "excedentes {'otra': 1}\n",
      "Limpiando caché de 0 objetos...\n",
      "Memoria: svmem(total=12649299968, available=7890378752, percent=37.6, used=4758921216, free=7890378752)\n"
     ]
    }
   ],
   "source": [
    "usados = {}\n",
    "excedentes = {}\n",
    "n_grama = \"Esto es una prueba\"\n",
    "evaluados = [\"Es\", \"to\", \"ta\", \"es\", \"una\", \"pru\", \"eba\", \".\", \" \"]\n",
    "n_grama = \"Esto es una prueba\"\n",
    "invent.explota_fragmento(n_grama, usados, excedentes, evaluados)\n",
    "n_grama = \"Esto es otra prueba\"\n",
    "invent.explota_fragmento(n_grama, usados, excedentes, evaluados)\n",
    "print(\"usados\", usados)\n",
    "print(\"excedentes\", excedentes)\n",
    "del usados\n",
    "del evaluados\n",
    "del excedentes\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluador de fragmentos en n-gramas para obtener tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_ttkn(a_ordenar):\n",
    "    global framentos, acum_fragmentos, fragmentos_n_grama, acum_frag_n_grama\n",
    "    return sorted(\n",
    "        a_ordenar,\n",
    "        key=lambda x: (\n",
    "            0.55 + len(x) * (1.0 + len(x) - len(set(x))) / max_len_frag\n",
    "        ) ** (\n",
    "            (\n",
    "                1.0 * (fragmentos[ttkn][x] / acum_fragmentos[ttkn]) +\n",
    "                3.0 * (len(fragmentos_n_grama[ttkn][x]) /acum_frag_n_grama[ttkn])\n",
    "            ) / 4.0\n",
    "        ),\n",
    "        reverse=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Evaluando fragmentos sobre tokens ...||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Ordenando 19731 fragmentos de Letras...Fragmentos previos: 19731 = ['ll', 'ara', 'ill', 'rr', 'ell', 'ada', 'lla', 'ala', 'arr', 'ana']\n",
      "               Optimizando 19731 n-gramas de Letras cada 198 un punto....................................................................................................!\n",
      "Fragmentos evaluados: 18676 = ['ll', 'ara', 'rr', 'ada', 'ala', 'ana', 'aba', 'ili', 'ama', 'ss']\n",
      "               Evaluando 573341 n-gramas de Letras cada 5734 un punto....................................................................................................!\n",
      "Recogiendo datos...\n",
      "Fragmentos usados en Letras: 14100\n",
      "Excedentes en Letras: 37\n",
      "Limpiando caché de 0 objetos...\n",
      "Ordenando 468 fragmentos de Separadores...Fragmentos previos: 468 = ['   ', '  ', '...', '.  ', ' . ', '..', ' , ', ',  ', '  .', ' ..']\n",
      "              Optimizando 468 n-gramas de Separadores cada 5 un punto..............................................................................................!\n",
      "Fragmentos evaluados: 378 = ['   ', '  ', '...', ' . ', '..', ' , ', ' : ', '. .', ' + ', '___']\n",
      "              Evaluando 2469 n-gramas de Separadores cada 25 un punto...................................................................................................!\n",
      "Recogiendo datos...\n",
      "Fragmentos usados en Separadores: 329\n",
      "Excedentes en Separadores: 29\n",
      "Limpiando caché de 0 objetos...\n",
      "Ordenando 1100 fragmentos de Dígitos...Fragmentos previos: 1100 = ['00', '000', '11', '22', '100', '33', '55', '44', '200', '66']\n",
      "                Optimizando 1100 n-gramas de Dígitos cada 12 un punto............................................................................................!\n",
      "Fragmentos evaluados: 910 = ['00', '11', '22', '33', '55', '44', '66', '99', '77', '88']\n",
      "                Evaluando 10350 n-gramas de Dígitos cada 104 un punto....................................................................................................!\n",
      "Recogiendo datos...\n",
      "Fragmentos usados en Dígitos: 910\n",
      "Excedentes en Dígitos: 10\n",
      "Limpiando caché de 0 objetos...\n",
      "Ordenando 84 fragmentos de Símbolos...Fragmentos previos: 84 = ['}}', '}}}', '&&&', '&&', '})}', '}})', ')}}', '}}{', '???', '??']\n",
      "                  Optimizando 84 n-gramas de Símbolos cada 1 un punto.....................................................................................!\n",
      "Fragmentos evaluados: 63 = ['}}', '&&&', '&&', '})}', '???', '??', '!!!', '}|}', '!!', ']]']\n",
      "                   Evaluando 410 n-gramas de Símbolos cada 5 un punto...................................................................................!\n",
      "Recogiendo datos...\n",
      "Fragmentos usados en Símbolos: 60\n",
      "Excedentes en Símbolos: 167\n",
      "Limpiando caché de 0 objetos...\n",
      "Ordenando 0 fragmentos de Desconocidos...Fragmentos previos: 0 = []\n",
      "               Optimizando 0 n-gramas de Desconocidos cada 1 un punto.!\n",
      "Fragmentos evaluados: 0 = []\n",
      "             Evaluando 5744 n-gramas de Desconocidos cada 58 un punto....................................................................................................!\n",
      "Recogiendo datos...\n",
      "Fragmentos usados en Desconocidos: 0\n",
      "Excedentes en Desconocidos: 5744\n",
      "Limpiando caché de 0 objetos...\n",
      "Ordenando 0 fragmentos de Especiales...Fragmentos previos: 0 = []\n",
      "                 Optimizando 0 n-gramas de Especiales cada 1 un punto.!\n",
      "Fragmentos evaluados: 0 = []\n",
      "                   Evaluando 3 n-gramas de Especiales cada 1 un punto....!\n",
      "Recogiendo datos...\n",
      "Fragmentos usados en Especiales: 0\n",
      "Excedentes en Especiales: 3\n",
      "Limpiando caché de 0 objetos...\n",
      "Limpiando caché de 0 objetos...\n",
      "Memoria: svmem(total=12649299968, available=7737417728, percent=38.8, used=4911882240, free=7737417728)\n"
     ]
    }
   ],
   "source": [
    "fragmentos_usados_ttkn = {ttkn: {} for ttkn in fragmentos}\n",
    "excedentes_ttkn = {ttkn: {} for ttkn in fragmentos}\n",
    "fragmentos_evaluados_ttkn = {}\n",
    "print(f\"Evaluando fragmentos sobre tokens ...\".rjust(70) + \"\".join([\"|\"]*100))\n",
    "for ttkn in dic_ngramas:\n",
    "    print(f\"Ordenando {len(fragmentos[ttkn])} fragmentos de {K.TTKN_DESC[ttkn]}...\", end=\"\")\n",
    "    fragmentos_usados = {}\n",
    "    excedentes = {}\n",
    "    previos = sorted_ttkn(fragmentos[ttkn])\n",
    "    print(f\"Fragmentos previos: {len(previos)} = {previos[:10]}\")\n",
    "    evaluados = []\n",
    "    cont = 0\n",
    "    cada = 1 + len(previos) // 100\n",
    "    print(f\"Optimizando {len(previos)} n-gramas de {K.TTKN_DESC[ttkn]} cada {cada} un punto.\".rjust(70), end=\"\")\n",
    "    no_esta = True\n",
    "    for fragmento in previos:\n",
    "        no_esta = True\n",
    "        for x in evaluados:\n",
    "            if len(x) < len(fragmento) and x in fragmento:\n",
    "                no_esta = False\n",
    "                break\n",
    "        if no_esta:\n",
    "            evaluados.append(fragmento)\n",
    "        cont += 1\n",
    "        if cont % cada == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    del previos\n",
    "    print(\"!\")\n",
    "    print(f\"Fragmentos evaluados: {len(evaluados)} = {evaluados[:10]}\")\n",
    "    cada = 1 + len(dic_ngramas[ttkn]) // 100\n",
    "    print(f\"Evaluando {len(dic_ngramas[ttkn])} n-gramas de {K.TTKN_DESC[ttkn]} cada {cada} un punto.\".rjust(70), end=\"\")\n",
    "    cont = 0\n",
    "    for n_grama in dic_ngramas[ttkn]:\n",
    "        if isinstance(n_grama, str) and len(n_grama) < 50:\n",
    "            invent.explota_fragmento(n_grama, fragmentos_usados, excedentes, evaluados)\n",
    "        cont += 1\n",
    "        if cont % cada == 0:\n",
    "            print(\".\", end=\"\")\n",
    "    print(\"!\")\n",
    "    print(\"Recogiendo datos...\")\n",
    "    fragmentos_usados_ttkn[ttkn] = fragmentos_usados.copy()\n",
    "    print(f\"Fragmentos usados en {K.TTKN_DESC[ttkn]}: {len(fragmentos_usados)}\")\n",
    "    excedentes_ttkn[ttkn] = excedentes.copy()\n",
    "    print(f\"Excedentes en {K.TTKN_DESC[ttkn]}: {len(excedentes)}\")\n",
    "    del fragmentos_usados\n",
    "    del excedentes\n",
    "    print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(f\"Limpiando caché de {gc.collect()} objetos...\")\n",
    "print(\"Memoria:\", psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado (si procede)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando D:\\datos\\tokens_usados.pkl\n",
      "Guardando D:\\datos\\tokens_excedentes.pkl\n"
     ]
    }
   ],
   "source": [
    "fragmentos_usados_path = r\"D:\\datos\\tokens_usados.pkl\"\n",
    "excedentes_path = r\"D:\\datos\\tokens_excedentes.pkl\"\n",
    "print(f\"Guardando {fragmentos_usados_path}\")\n",
    "with open(fragmentos_usados_path, \"wb\") as file:\n",
    "    pickle.dump(fragmentos_usados_ttkn, file)\n",
    "print(f\"Guardando {excedentes_path}\")\n",
    "with open(excedentes_path, \"wb\") as file:\n",
    "    pickle.dump(excedentes_ttkn, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diccionario de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orden y numeración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttkn_start_id = {\n",
    "    K.TTKN_ESP: 50,\n",
    "    K.TTKN_SEP: 1000,\n",
    "    K.TTKN_SIM: 5000,\n",
    "    K.TTKN_DIG: 20000,\n",
    "    K.TTKN_LET: 40000,\n",
    "    K.TTKN_UNK: 90000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para Especiales empezamos por 50, usados (49), excedentes (52), monogramas (52), y constantes. Terminamos por 57. Tenemos 8\n",
      "Llevamos 17\n",
      "Para Separadores empezamos por 1000, usados (1328), excedentes (1357), monogramas (1357), y constantes. Terminamos por 1358. Tenemos 359\n",
      "Llevamos 376\n",
      "Para Símbolos empezamos por 5000, usados (5059), excedentes (5226), monogramas (5226), y constantes. Terminamos por 5226. Tenemos 227\n",
      "Llevamos 603\n",
      "Para Dígitos empezamos por 20000, usados (20909), excedentes (20919), monogramas (20919), y constantes. Terminamos por 20919. Tenemos 920\n",
      "Llevamos 1523\n",
      "Para Letras empezamos por 40000, usados (54099), excedentes (54136), monogramas (54136), y constantes. Terminamos por 54136. Tenemos 14137\n",
      "Llevamos 15660\n",
      "Para Desconocidos empezamos por 90000, usados (89999), excedentes (95743), monogramas (95743), y constantes. Terminamos por 95743. Tenemos 5744\n",
      "Llevamos 21404\n"
     ]
    }
   ],
   "source": [
    "# Empezamos por los tokens del sistema\n",
    "tokens = {}\n",
    "tokens[K.TTKN_SIS] = {token:K.MAIN_TOKEN_DICT[token] for token in K.MAIN_TOKEN_DICT}\n",
    "# Inicializamos los tipos de token previendo su rellenado posterior\n",
    "for ttkn in fragmentos:\n",
    "    tokens[ttkn] = {}\n",
    "# Iniciamos desde el token_id = 10 como minimo\n",
    "token_id = 10\n",
    "# A rellenar...\n",
    "for ttkn in ttkn_start_id:\n",
    "    if ttkn not in fragmentos_usados_ttkn:\n",
    "        continue\n",
    "    # Situamos la numeracion optimizada\n",
    "    if token_id < ttkn_start_id[ttkn]:\n",
    "        token_id = ttkn_start_id[ttkn]\n",
    "    else:\n",
    "        token_id += ttkn_start_id[ttkn]\n",
    "    print(f\"Para {K.TTKN_DESC[ttkn]} empezamos por {token_id}\", end=\",\")\n",
    "    # Incluimos en esa trancha de ids los tokens\n",
    "    for fragmento in sorted_ttkn(fragmentos_usados_ttkn[ttkn]):\n",
    "        tokens[ttkn][fragmento] = token_id\n",
    "        token_id += 1\n",
    "    print(f\" usados ({token_id-1}),\", end=\"\")\n",
    "    for excedente in excedentes_ttkn[ttkn]:\n",
    "        tokens[ttkn][excedente] = token_id\n",
    "        token_id += 1\n",
    "    print(f\" excedentes ({token_id-1}),\", end=\"\")\n",
    "    # Que no se nos olviden los monogramas encontrados\n",
    "    if ttkn in prob_monogramas:\n",
    "        for monograma in sorted(prob_monogramas[ttkn], key=lambda x: prob_monogramas[ttkn][x], reverse=True):\n",
    "            if monograma not in tokens[ttkn]:\n",
    "                tokens[ttkn][monograma] = token_id\n",
    "                token_id += 1\n",
    "    print(f\" monogramas ({token_id-1}),\", end=\"\")\n",
    "    # Para que quede todo relleno, los de las constantes\n",
    "    if ttkn in K.TIPOS_TKN:\n",
    "        for caracter in K.TIPOS_TKN[ttkn]:\n",
    "            if caracter.lower() not in tokens[ttkn]:\n",
    "                tokens[ttkn][caracter.lower()] = token_id\n",
    "                token_id += 1\n",
    "    print(f\" y constantes. Terminamos por {token_id-1}. Tenemos {len(tokens[ttkn])}\")\n",
    "    print(f\"Llevamos {sum([len(tokens[x]) for x in tokens])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardado de Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardando D:\\datos\\tokens_ttkn_id_v20241208.pkl\n"
     ]
    }
   ],
   "source": [
    "tokens_path = r\"D:\\datos\\tokens_ttkn_id_v20241208.pkl\"\n",
    "print(f\"Guardando {tokens_path}\")\n",
    "with open(tokens_path, \"wb\") as file:\n",
    "    pickle.dump(tokens, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
